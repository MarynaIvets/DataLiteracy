[["index.html", "A concise guide to reproducible research using secondary data Introduction Objective Learning objectives Structure of the study guide This is a living document", " A concise guide to reproducible research using secondary data Katharina Blankart, Maryna Ivets, Eva Goetjes, Kai Miele 2022-03-22 Introduction Objective This study guide is a resource for graduate students, PhD candidates, and researchers performing applied empirical research in economics and management sciences. The guide is meant for the field of analysis of health care markets using secondary data. Many textbook examples use readily available datasets for analysis of econometric problems. For students developing a related research question and generating their own analysis dataset, important steps that lead to a final analysis dataset are often missing. Additionally, many resources focus on labor economics problems. Resources that showcase processing and generating secondary data are scarce. One reason is that data sources used in health care applications are often subject to confidentiality and data protection issues. This guide explains the five essential steps needed to create a reproducible research project. We introduce important terminology, highlight relevant tasks, and provide key resources in the form of textbooks and websites available via open access. We provide a concise guide that users can easily access when starting academic research. Each section takes about 10 to 15 minutes to read. We do not cover any specific data science or econometric method, but point to the relevant resources. To use this guide most efficiently, users are required to have basic knowledge in statistics, econometrics and program evaluation methods. Users should be familiar with one essential programming language and one major statistical package such as R or Stata. For maximum benefit readers should have background knowledge and a research idea in mind for their own reproducible project. Learning objectives The goal is to set up and carry out a data science project using secondary data. Students will learn all steps starting with hypothesis formulation, data generation and analysis, and presentation of empirical results. After reading and applying the principles introduced in this study guide, you will be able to: Recognize the features of using secondary (health care) data in empirical research. Execute the steps of a reproducible research project. Implement an empirical research project. Recall the steps taken to execute a reproducible research project using secondary data. Structure of the study guide The study guide consists of five chapters that include the essential steps of a reproducible research project. Each step is covered in three parts. An introduction to the basic concepts and key terminology. A resources box that includes textbooks, articles and references to current web resources with emphasis on open access material. A checklist for each step of the reproducible research project to follow. A showcase example of an empirical project replicated based on the article of Hellerstein, Judith K. 1998. The Importance of the Physician in the Generic versus Trade-Name Prescription Decision. The RAND Journal of Economics 29 (1): 10836. https://doi.org/10.2307/2555818. This is a living document How can you contribute to this study guide? Best practices how to perform reproducible research are constantly developing. We aim to keep resources up to date. If you come across good resources that serve as additions, preferably open access, or have suggestions for improvement, please share them by emailing to: katharina.blankart@uni-due.de "],["about-the-authors.html", "About the authors", " About the authors Katharina Blankart (katharina.blankart@uni-due.de, katblankart.github.io) is an assistant professor in empirical health economics at the Faculty of Economics and Business Administration at the University of Duisburg-Essen and CINCH Health Economics Research Center. Katharina is an economist with a PhD in Business Administration (both Ludwig-Maximilians University Munich, Germany). Her research studies the institutional settings of determining value of and access to health technology. An important aspect of this research is how providers and patients use technology to improve health and health care. Katharinas research emphasizes the health policies and management of pharmaceutical care using secondary data. Maryna Ivets (maryna.ivets@uni-due.de) is a Ph.D. candidate in economics at the university of Duisburg-Essen, Germany. She is an applied economist with expertise in secondary data analysis in the fields of behavioral, health, and education economics. Her research uses psychology and economics to improve our understanding of early childcares effects on development. She is also researching how to encourage positive health behaviors through novel mechanisms, such as online self-challenges. Eva Goetjes [ please fill in] Kai Robert Miele (kai.miele@ibes.uni-due.de) is a masters student in economics and an upcoming doctorate candidate at University Duisburg-Essen. Kai specialized his studies in applied microeconomics in a health and labour economic context. In one specific project, Kai analyses mental health effects of financial access to education in adulthood. Future projects are planned to cover economic returns and later-in-life health benefits of different education systems in Western countries. "],["acknowledgements.html", "Acknowledgements", " Acknowledgements The authors thank Christoph Kronenberg for comments and suggestions. Development of this resource has received funding by Data Literacy Education.nrw. It is part of the DataCampus project of the University of Duisburg-Essen. This concise guide to reproducible research using secondary data is written as an Open Educational Resource to enable you to use this book in the best possible way. This work is therefore made available under Creative Commons Public Domain Dedication (CC0 1.0 Universal). You do not have to ask us a permission to re-use and copy information from this handbook. Take note that some materials cited in this book might be copyright protected. If so, this is indicated in the text. Please consider citing the handbook when using the content. We have tried to acknowledge all of our sources. If for some reason we have forgotten to provide you with proper credits it has not been done with malicious intent. Feel free to contact us at katharina.blankart@uni-due.de for any corrections. "],["intro.html", "Chapter 1 Introduction to Reproducible Research 1.1 What is reproducible research? 1.2 Why you, as a student and researcher, should care about reproducible research? 1.3 Main steps to generate reproducible research 1.4 Secondary (health care) data 1.5 Creating an environment for productive research projects 1.6 Resources Box 1.7 Checklist to get started with your reproducible project", " Chapter 1 Introduction to Reproducible Research Only results that can be replicated are truly scientific results. If there is no chance to replicate research results, they can be regarded as no more than personal views in the opinion or review section of a daily newspaper. (Huschka 2013) 1.1 What is reproducible research? Scientific journal editors and research funders are increasingly promoting transparency in research. To encourage the principles of reproducible research, the related institutions are requesting authors to make their research reproducible. To overcome criticisms regarding the validity and power of empirical tests, this means that data and program code need to be shared upon manuscript acceptance, or at earlier stages of the submission process. The purpose is that third-parties have the possibility to reproduce the content, analysis, and conclusions of a study on their own. Efforts to increase reproducability have been expressed by institutions within the social sciences including: The best practices statement by the Social Science Data Editors, In Germany, by the German Research Foundation (DFG) and the Consortium for the Social, Behavioural, Educational and Economic Sciences RatWSD In editorial statement of journals, for example, American Economic Review, Management Science or the Journal of International Business Studies (Meyer, Witteloostuijn, and Beugelsdijk 2017; Orozco et al. 2020) Generally, reproducibility of research can be defined as the ability of a researcher to duplicate the results of a prior study using the same materials and procedures as were used by the original investigator. In an attempt to reproduce a published statistical analysis, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis to determine whether they yield the same results. (Bollen et al. 2015). Study results are considered reproducible if after an article publication another researcher can conduct the analyses using identical data and obtain the same results using the material provided. As empirical research is based on the application of a code to a dataset to answer a pre-defined research question, ensuring the reproducibility includes sharing the data and code to allow others to re-analyze the data and to reproduce the reported results (Orozco et al. 2020). To achieve this, the data and code need to be properly managed while working on a project. Another concept closely related to reproducibility is the concept of replicability that refers to the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected. (Bollen et al. 2015). Thus, for example, if an investigator tries to replicate a scientific finding that documents a relationship between two or more variables by using the same scientific methodology but in a new setting, i.e. with new data, and fails to reach a similar conclusion, i.e. replicate it  a failure to replicate occurs. The opposite is said to be true if the results are replicated. Thus, reproducibility and replicability are considered to be the two main elements of empirical research. To adapt reproducible practices early on, undergraduate and graduate students are encouraged to perform reproducible research in their term papers, theses, and practical applications as soon as possbile. For this reason, university teachers are increasingly asking to submit reproduction material (source data information, data programming and analysis code) at all stages of a study. The concept of reproducible research is not new and goes back as far as the late 1800s (Vilhuber 2020). However, reproducibility studies have disclosed that many rearchers do not follow the principles of reproducible research. At the same time, the increase in availability and use of public and especially non-public data sources, and the increase in the reliance of research methods based on specific software brings the principles of reproducible research on the agenda of many researchers. The resources boxes in each chapter provide material for best practices in data and code management to perform a reproducible research project, as well as in cleaning the data and conducting data analysis. 1.2 Why you, as a student and researcher, should care about reproducible research? 1.2.1 Avoid common biases that lead to biased or false research results Figure 1.1: Threats to reproducible science There is a number of threats to the reproducible research process that can undermine scientific research or lead to false or biased conclusions and publications. Figure 1.1 illustrates the main threats to reproducible science (Munafò et al. 2017). One of the most fundamental threats when using secondary data is hidden researcher bias which occurs when decisions about data collection, preparation and analysis influence data selection and other steps in such a way that the identified empirical effects considerably change from these decisions (Huntington-Klein et al. 2021). 1.2.2 Increase productivity of your work and the work of the scientific community by performing reproducible research Following the reproducible research principles allows you to be a part of a good academic practice that strives to improve the quality, efficiency and reliability of scientific research. 1.3 Main steps to generate reproducible research To follow the best practices of reproducible research, you will need to consider how to perform your project and the steps a reproducible research project contains. You may follow three main principles to enhance reproducible research (Orozco et al. 2020): Organize your work: consider and plan your steps at the beginning of the project Code for others: set up each step of your project such that an outsider could follow your documentation Automate as much as you can: avoid processing analyses and results using point-and-click softare (MS Excel), export results directly and create a reproducible project documentation. Figure 1.2: Reproducible research In this study guide we follow the main steps of performing a reproducible data science project (Bezjak et al. 2018). To make your empirical research study process reproducible, you need to follow these five steps (Figure 1.2): Formulating a hypothesis (Section 2) Designing the study (Section 3) Running the study and collecting the data (Section 4) Analyzing the data (Section 5) Reporting the study (Section 6) 1.4 Secondary (health care) data This study guide concentrates on empirical investigation using secondary data, that means data that is not originally collected or generated by the researcher for the purpose of the study. Secondary data covers any existing data generated by companies, institutions, and individuals. Examples of secondary data sources are routinelly collected health data (administrative claims, electronic medical records), bibliometric data, survey data, regulatory data, data generated in mobile applications. 1.5 Creating an environment for productive research projects 1.5.1 Consider using agile methods to organize your workflow and tasks To set up your project, consider engaging project management tools. One suitable method is working agile or based on SCRUM methods (Pirro 2019; Santiago-Lopez 2019). Originating in software development, working agile is highly suitable in performing research projects. A planning protocol could include the following steps (as adapted from (Pirro 2019)). Split the work. Slice a big chunk of work into several layers of activities, in which each layer is characterized by a tangible result to be obtained.Each layer is addressed in a dedicated, limited period of time (for example, 212 weeks), called a sprint. Sprint planning. Meet your supervisor and any other stakeholders in a short meeting (around 30 minutes) with the aim of defining the goal of your sprint (for example, what you want to investigate) and its duration (1-4 weeks, for instance). Everybody has to agree on these two points, so that expectations are aligned and the whole research team is on the same page. On this occasion, the sprint-review meeting (see step five) can be scheduled. Sprint execution. Work! Maximum focus is required on a specific task for a limited amount of time. You can do it, keep momentum. Weekly scrum. Meet your supervisor for a maximum of 15 minutes, but ideally every week (for example, the same time slot every week and outside of conventional working hours to ensure there are no commitments, such as meetings or teaching activities, to get in the way). This meeting has to be short and efficient  try to have a stand-up meeting with no laptops or papers. Only three questions need to be addressed: what was done the previous week to contribute to the goal? (For example, which data programs were developed? Which analyses were performed) What will be done next week to contribute to the goal? (For example, what analyses will be performed next?) And, are there any impediments? (For example, is the set-up working properly? Are all the materials and data needed available?) Sprint review, retrospective and planning. At the end of the sprint, meet all of the stakeholders to discuss results and whether those are in line with expectations (review). Take some time to go into detail and do some analytical brainstorming together. Discuss the difficulties encountered, so that the next sprint is better than the previous one (retrospective). This is the phase for impediment removal, or problem solving. Honesty and transparency are crucial. Agile is all about adapting to change: plans can change. Go back to step one and restart the planning, addressing the next layer of work in a new sprint. You can execute the same routine even if you work idepenently. 1.5.2 Collaborate with others and get feedback Collaborate: Before you start running the study you should assess your available resources, that means time and own expertise to see whether the study design is feasible. If there is an opportunity to work collaboratively and you think that the project can benefit from the expertise of another researcher, you can approach the person and ask whether they would be interested to work together on the project. Joining resources might relax your resource availability contraints. It may further ensure that you engage in a reproducible research process. Plan to get feedback - create your personal senior advisory board: Besides your collaborators, set up your personal senior advisory board of the project. This could be anybody with experience related to your research project (other students, PhD candiates). Define when you plan to meet your advisors and present the progress of your work. Define the specific competencies with which seniors can help you with your project. This could include advising on study design, methods, programming or interpretation of the results. If you work on a longer-term project, find suitable workshops and conferences to present your work and get feedback. Do not forget to account for time and resources to proof-read and edit your research findings in a paper. 1.6 Resources Box An overview article of the principles of reproducible reserach and practical application: Orozco V, Bontemps C, Maigné E, Piguet V, Hofstetter A, Lacroix A, et al. How to Make a Pie: Reproducible Research for Empirical Economics and Econometrics. Journal of Economic Surveys. 2020;34(5):113469. https://doi.org/10.1111/joes.12389 A guideline to minimize the risk of reporting false positives (type I errors), improve the quality of hypothesis-testing research and statistical reporting: Meyer, Klaus E., Arjen van Witteloostuijn, and Sjoerd Beugelsdijk. 2017. Whats in a p? Reassessing Best Practices for Conducting and Reporting Hypothesis-Testing Research. Journal of International Business Studies 48 (5): 53551. https://doi.org/10.1057/s41267-017-0078-8. The Worldbank blog provides A Curated List of Our Postings on Technical Topics  Your One-Stop Shop for Methodology 1.7 Checklist to get started with your reproducible project Define the resources available and needed to perform your reproducible research project. Set up a working plan that includes important milestones and minimal goals to achieve. Decide for a project management method to organize your work, for example agile methods or SCRUM. Define the tools and software for management of references, notes and editing of text and statistical software you will be using. Obtain access if needed. Define relevant collaborators and stakeholders critical in persuing your reserach project. Define important milestones and achievements. References "],["hypothesis.html", "Chapter 2 Formulating a hypothesis 2.1 How do you develop a research question and formulate a hypothesis? 2.2 What is a hypothesis? 2.3 Resources box 2.4 Checklist to get started with formulating your hypothesis 2.5 Example: Hellerstein (1998)", " Chapter 2 Formulating a hypothesis There is no single best way to develop a research idea. (Pischke 2012) 2.1 How do you develop a research question and formulate a hypothesis? You decide to undertake a scientific project. Where do you start? First, you need to find a research question that interests you and formulate a hypothesis. We will introduce some key terminology, steps you can take, and examples how to develop research questions. Note that . What if someone assigns a topic to me? For students attending undergraduate and graduate courses that often pick topics from a list, all of these steps are equally important and necessary. You still need to formulate a research question and a hypothesis. And it is important to clarify the relevance of your topic for yourself. When thinking about a research question, you need to identify a topic that is [based on [find source]] Relevant, important in the world and interesting to you as a researcher: Does working on the topic excites you? You will spend many hours thinking about it and working on it. Therefore, it should be interesting and engaging enough for you to motivate your continued work on this topic. Specific: not too broad and not too narrow Feasible to research within a given timeframe: Is it possible to answer the research question based on your time budget, data and additional resources. How do you find a topic or develop a feasible research idea in the first place? Finding an idea is not difficult, the critical part is to find a good idea. How do you do that? There is no one specific way how one gets an idea, rather there is a myriad of ways how people come up with potential ideas (for example as stated by (Varian 2016)). You can find inspiration by Looking at insights from the world around you: your own life and experiences, observe the behavior of people around you Talking to people around you, experts, other students, family members Talking to individuals outside your field (non-economists) Talking to professionals working in the area you are interested in (you may use social media and professional platforms like LinkedIN or Twitter to make contact) Reading journal articles from other non-economic social sciences and the medical literature Reading economic and non-economic newspapers and magazine articles (e.g., Süddeutsche, Frankfurter Allgemeine, Tagesspiegel, The Economist, the Wall Street Journal, The New York Times, The Guardian), watching TV programs What are the issues being discussed? How do these issues affect peoples lives? In addition you could Go to virtual and in-person seminars, for example, the Essen Health Economics Seminar Look at abstracts of scientific articles and working papers Look at the literature in a specific field you are interested in, for example, screening complete issues of journals or editorials about certain research advancements. By reading this literature you might come up with the idea on how to extend and refine previous research. Once you identified a research question that is of interest to you, you need to define a hypothesis. 2.2 What is a hypothesis? A hypothesis is a statement that introduces your research question and suggests the results you might find. It is an educated guess. You start by posing an economic question and formulate a hypothesis about this question. Then you test it with your data and empirical analysis and either accept or reject the hypothesis. It constitutes the main basis of your scientific investigation and you should be careful when creating it. 2.2.1 Develop a hypothesis Before you formulate your hypothesis, read up on the topic of interest. This should provide you with sufficient information to narrow down your research question. Once you find your question you need to develop a hypothesis, which contains a statement of your expectations regarding your research questions results. You propose to prove your hypothesis with your research by testing the relationship between two variables of interest. Thus, a hypothesis should be testable with the data at hand. There are two types of hypotheses: alternative or null. Null states that there is no effect. Alternative states that there is an effect. There is an alternative view on this that suggests one should not look at the literature too early on in the idea-generating process to not be influenced and shaped by someone elses ideas ((Varian 2016).). According to this view you can spend some time (i.e. a few weeks) trying to develop your own original idea. Even if you end up with an idea that has already been pursued by someone else, this will still provide you with good practice in developing publishable ideas. After you have developed an idea and made sure that it was not yet investigated in the literature, you can start conducting a systematic literature review. By doing this, you can find some other interesting insights from the work of others that you can synthesize in your own work to produce something novel and original. 2.2.2 Identify relevant literature For your research project you will need to identify and collect previous relevant literature. It should involve a thorough search of the keywords in relevant databases and journals. Place emphasis on articles from high-ranking journals with significant numbers of citations. This will give you an indication of the most influential and important work in the field. Once you identify and collect the relevant literature for your topic, you will need to critically synthesize it in your literature review. When you perform your literature review, consider theories that may inform your research question. For example, when studying physician behavior you may consider principal-agency theory. 2.2.3 Research question or literature review: the chicken or the egg problem? Whether you start reading the literature first or by developing an idea may depend on your level (graduate student, early career researcher) and other goals. However, thinking freely about what you like to investigate first may help to critically develop a feasible and interesting research question. We highlight an example how to start with investigating the real world and subsequently posing a research question (How to Write a Strong Hypothesis Steps and Examples 2019; Developing Strong Research Questions Criteria and Examples 2019; Schilbach 2019). For example, based on your observation you notice that people spend extensive amount of time looking at their smartphones. Maybe even you yourself engage in the same behavior. In addition, you read a BBC News article Social media damages teenagers mental health, report says. (#fig:social_media)Social media and mental health Source: BBC You decide to translate this article and your observations into a research question: How does social media use affect mental health? Before you formulate your hypothesis, read up on the topic of interest. Read economic, medical and other social science literature on the topic. There is likely to be a vast amount of literature from non-economic fields that are doing research on your topic of interest, for example, psychology or neuroscience. Familiarize yourself with it and master it. Do not get distracted by different scientific methodologies and techniques that might seem not up-to-par to the economic studies (small sample sizes, endogeneity, uncovering association rather than causation, etc.), but rather focus on suggestions of potential mechanisms. A hypothesis is then your research question distilled into a one sentence statement, which presents your expectations regarding the results. You propose to prove your hypothesis by testing the relationship between two variables of interest with the data at hand. There are two types of hypotheses: alternative or null. The null hypothesis states that there is no effect. The alternative hypothesis states that there is an effect. A hypothesis related to the above-stated research question could be: The increased use of social media among teenagers leads to (is associated with) worse mental health outcomes, i.e. increased incidence of depression, eating disorders, worse well-being and lower self-esteem. It suggests a direction of a relationship that you expect to find that is guided by your observations and existing evidence. It is testable with scientific research methods by using statistical analysis of the relevant data. Your hypothesis suggests a relationship between two variables: social media use (your independent variable \\(X\\)) and mental health (dependent variable \\(Y\\)). It could be framed in terms of correlation (is associated with) or causation (leads to). This should be reflected in the choice of scientific investigation you decide to undertake. The null hypothesis is: There is no relationship between social media use among teenagers and their mental health. 2.3 Resources box 2.3.1 How to develop strong research questions The form of the research process Varian, H. R. (2016). How to build an economic model in your spare time. The American Economist, 61(1), 81-90. 2.3.2 Identify relevant literature from major general interest and field literature To identify the relevant literature you can use academic search engines such as Google Scholar, Web of Science, EconLit, PubMed. search working paper series such as the National Bureau of Economic Research, NetEc or IZA search more general resource sites such as Resources for Economists go to the library/use library database 2.3.3 Assess the quality of a journal article Several rankings may help to assess the quality of research you consider Journals of general interest and by field in economics and management - For German-speaking countries, consider the VWL / BWL Handelsblatt Ranking for economics and management - The German Association of Management Scholars provides an expert-based ranking VHB JourQual 3.0, Teilranking Management im Gesundheitswesen - Web of Science Impact Factors - Scimago Health Economics, Health Services and Health Care Managment Research: Health Economics Journals List Be aware that like in any other domain there are predatory publishing practices. Use tools to investigate how a journal article is connected to other works Citationgecko Connected papers scite_  a tool to get a first impression whether a study is disputed or academic consensus 2.3.4 Organize your literature Zotero (free of charge) Mendeley (free of charge) EndNote (potentially free of charge via your university) Citavi (potentially free of charge via your university) BibTEX if you work with TEX Excel spread sheet 2.4 Checklist to get started with formulating your hypothesis Find an interesting and relevant research topic, if not assigned Try to suck up all information you can easily obtain from various sources within and outside academic literature Formulate one compelling research question Find the best available empirical and theoretical evidence that is related to your research question Formulate a hypothesis Check whether data are available for analysis Challenge your idea with your fellows or senior researchers 2.5 Example: Hellerstein (1998) As an illustration of the research process of formulating a hypothesis, designing a study, running a study, collecting and analyzing the data and, finally, reporting the study, we provide an example by replicating Judith K. Hellersteins paper The Importance of the Physician in the Generic versus Trade-Name Prescription Decision that was edited in 1998 in the The RAND Journal of Economics. Hellersteins 1998 paper has impacted discussion about behavioral factors of physician decisions and pharmaceutical markets over two decades. The study received 446 citations on Google Scholar since 1998 by 22/02/2022, including recent mentions in top field journals such as Journal of Public Economics (2021), Journal of Health Economics (2019), and Health Economics (2019). Figure 2.1: Connected graph of Hellerstein (1998), February 2022 Figure 2.1 shows a connected graph of prior and derivate works related to the study. The work has impacted the literature researching the role of physician behavior and its influence on access, adoption and diffusion of health services, moral hazard and incentives in prescription and treatment decisions and the influence of different payment schemes, and a vast body of literature studying the pharmaceutical market. The research that has been influenced by Hellerstein includes evidence on: generic drug entries and market efficiency the effectiveness of pharmaceutical promotion the effectiveness of price regulations the role of patents and dynamics of market segmentation At the end of each chapter, we demonstrate insights into this study that we replicated. 2.5.1 Context of the study - escalating health expenditures In the United States, the total prescription drug expenditure in 2020 marked about 358.7 billion US Dollars (Statista n.d.). The prescription of generic drugs in comparison to more expensive brand name versions is an option in reducing the total health care expenditure. Generic drugs are bioequivalent in the active ingredients and can serve as a channel to contain prescription expenditure (Kesselheim 2008) as generic drugs are between 20 and 90% cheaper than their trade-name alternatives (Dunne et al. 2013). 2.5.2 Research question - How does a patients insurance status influence the physicians choice between generic compared to brand-name drugs? Physicians are faced with a multitude of medication options, including the choice between generic and trade-name drugs. Physicians ideally act as agents for their patients to identify the best available treatment option based on their needs. Choosing the best treatment entails cost of coordination and cognition. The prescription of generic drugs may serve as an example to what extent physicians customize treatments according to patients needs with regards to cost. From an economic point of view we may expect that once a generic drug is available, a perfectly rational agent (i.e. physician) would prescribe a generic drug instead of the trade-name version if therapeutically identical (Dranove 1989). This leads to the following research question: Do physicians vary their prescription decisions on a patient-by-patient basis or do they systematically prescribe the same version, trade-name or generic, to all patients?. The 1998 Hellersteins study examines two hypotheses: The physician prescribing choice influences the selection of a generic over a brand-name drug The patients insurance status influences the physicians choice between generic and brand-name drugs. For the purpose of this example and in the replication exercise we focus on the second aspect. 2.5.3 Hypothesis The paper formulates the following hypothesis: Physicians are more likely to prescribe generics to patients who do not have insurance coverage for prescription pharmaceuticals (moral hazard in insurance) Hellerstein (1998) discusses that, based on insurance status, some patients may demand certain care more than others. If, for example, the prescription drug is reimbursed by the patients health insurance, this may cause overconsumption. This behavior can potentially differ by the patients insurance scheme. A patient that has no insurance and, thus, does not get any reimbursement for prescription drugs, might have a higher incentive to demand cheaper generic drugs (Danzon and Furukawa 2011) than a patient with insurance that covers prescription drugs, either generic or trade-name. Given that the United States have different insurance schemes with varying prescription drug coverage, it is of interest to investigate the role of a patients insurance status in the physicians choice between generic compared to brand-name drugs. Hellerstein (1998) considers a patients insurance status as a matter of dividing the study population in groups for which the choice between generic and brand-name drugs differs. She suggests that There is a relationship between the prescription of a generic drug and insurance status of a patient. (Hellerstein 1998). Providing answers to a research question requires formulating and testing a hypothesis. Based on logic, theory or previous research, a hypothesis proposes an expected relationship within the given data. According to her research question, Hellerstein hypothesizes that: Physicians are more likely to prescribe generics to patients who do not have insurance coverage for prescription pharmaceuticals. Specifically, she writes if there is moral hazard in insurance when it comes to physician prescription behavior, there will be differences in the propensity of physicians to prescribe low-cost generic drugs, and these differences will be (partially) a function of the insurance held by the patient. In particular, if moral hazard exists, patients with extensive insurance coverage for prescription drugs (like those on Medicaid in 1989) should receive prescriptions written for generic drugs less frequently than patients with no prescription drug coverage. (Hellerstein 1998, 113) Based on Hellersteins considerations, we expect the effect of the insurance status on whether a patient receives a generic to be different from zero. To obtain a testable null hypothesis, we reformulate this relationship so that we reject the hypothesis if our expectations are correct. This means, if we expect to see an effect of insurance on prescriptions of generics, our null hypothesis is that insurance status has no effect on the outcome (prescription of generic drugs). No moral hazard arises from having obtained insurance. References "],["designstudy.html", "Chapter 3 Designing the Study 3.1 Basic steps in designing your study 3.2 Resources box 3.3 Checklist to get started with designing your study 3.4 Example: Hellerstein (1998)", " Chapter 3 Designing the Study 3.1 Basic steps in designing your study You have chosen your research question and formulated a hypothesis. You have collected and screened relevant literature on the topic. Now, you need to decide on your research design. Are you planning to uncover associations, or do you want to examine a causal relationship between your variables of interest (Pearl 2009; Pearl, Glymour, and Jewell 2016)? We will focus on investigating causal relationships and analyzing counterfactual situations. To translate direct observations in data to investigate cause-and-effect relationships, you will need to rely on a workable model that describes the elements and relationships of concepts reflected in your hypothesis. It is strongly recommended that you first describe the causal structures of the elements that you are studying, including any other observed or unobserved structures that may disturb the cause-and-effect relationship under investigation. The three rungs of the ladder of causality can be shortly described as adapted from Sobolev Spaces Rung Queries and targets Level 1: ASSOCIATIONS Query: Is the outcome more common in one treatment group than the other? Target: Difference between groups Level 2: EFFECTS OF CAUSES Query: How would outcomes differ if treatment groups were made up of the same subjects (individuals, neighborhoods, firms)? ; Target: Average treatment effect Level 3: CAUSAL ATTRIBUTION - COUNTERFACTUALS CAUSATION Query: How likely is it that treatment is necessary and sufficient cause of the outcome?; Target: Necessary and sufficient causes MEDIATION Query: What part of the average treatment effect is not mediated by other treatment outcomes?; Target: Natural direct effect PERSONALIZED EFFECT How prevalent is responding differently to different treatments?; Target: The probability of causation bounds Two approaches may be used to describe the causal relationships and the estimable effects that you may identify as part of your research question: Potential outcomes framework or PO-framework Directed acyclic graphs Both approaches are the powerhouses to performing empirical analyses to specify a) the effects that you aim to identify and b) the structural relationships between the concepts (dependent variables, variables of interests and confounders) under evaluation and c) whether your results allow for causal inference given your data. Both approaches are powerful and comprehensive. We keep this section short and refer to important resources in this field below. The design of your study will depend on the causal model that suggests which effects are estimable, given that there are suitable data to empirically identify the effect. Consider that you describe cause-and-effect relationship first, assess how it is estimable (that means that you ask the question Can you infer a causal effect from your data?) and then investigate the effect by direct observations using secondary data. 3.1.1 Potential Outcomes Framework The potential outcomes framework uses the potential values of the outcomes given a certain treatment (Abadie and Cattaneo 2018) to describe the causal parameters that you aim to identify in your empirical analysis and the type of effect. The PO-framework is particularly suited for evaluating the effect of policies (governmental, firm policies). In short, you would like to compare outcomes given a subject (individual, firm, region) has received a treatment to the outcomes when the subject has not received a treatment. The difference between the two will tell you the treatment effect from having received the treatment or policy under investigation by defining potential and realized outcomes. Using the potential outcomes framework helps you to specify and assess whether you can conclude that your estimands are coming from a population where the treatment has been randomly assigned to subjects. In the most simplest notation consider that there is a value of the treatment by the (random) variable \\(X\\), or often denoted as \\(W\\) or \\(D\\). We largely borrow from Abadie and Cattaneo (2018)s annotation. Consider that \\(X\\) has two values \\(1\\) that stands for that a subject has received a treatment and \\(0\\) which stands for that the individual has not received the treatment. The goal is to investigate the effect of changes in \\(X\\) on some observed outcomes variable, denoted as \\(Y\\). \\(Y_1\\) represents the potential value of the outcome when the value of the treatment variable \\(X\\) is set to \\(1\\). \\(Y_0\\) represents the potential value of the outcome when the subject does not receive the treatment. The causal effect of the treatment can be represented by the difference in potential outcomes, \\(Y_1-Y_0\\). The potential outcomes and the value of the treatment define the observed outcome \\(Y=XY_1+(1-X)Y_0\\) Note that we typically cannot observe both potential outcomes in the same subject which refers to as the fundamental problem of causal inference. To be able to make causal statements, we need to make additional assumptions to be able to estimate the average treatment effect (ATE),i.e. \\(\\tau_{ATE}=E[Y_1-Y_0]\\) This is a very short representation of the PO-Framework and we refer the reader for the additional assumptions of identifiablity to the resources box below. 3.1.2 Directed Acyclic Graphs One way of describing your causal structure is by describing causal relationship in directed acyclic graphs (DAG). A DAG is a systematic representation of a causal relationship between two (or more) variables (\\(X\\) and \\(Y\\)) with a focus on minimizing statistical bias by potential confounding variables (\\(Z\\)). It is recommended to do this step before data collection. That way, you avoid collection of data that may not be needed. You also avoid forgetting variables that are necessary for causal effect identification. And you may check for so-called bad controls that may bias your estimates. Figure 3.1: Directed Acyclic Graph 3.1.3 Develop a causal model and identify estimable effects We focus on study designs that use secondary data with the aim to identify the magnitude of effects of causal relationships between a variable of interest \\(X\\) and a certain outcome \\(Y\\), controlling for potential confounders \\(Z\\). Note that there is extensive literature and guidance about designing and performing causal inference studies, some of which we guide you to below. We will highlight the most important elements and point to the relevant resources. Consider the subsequent steps as an iterative process, allowing that not all data you may need to estimate effects of your causal model will be readily available. Some elements will remain unobserved. For others, you may need to find proxies. You should specify a causal model that describes the relationships between the elements that you are studying. This causal model will help to identify an empirical model by defining your main relationship of interest. Generally speaking, you need to specify your main outcome variable of interest (\\(Y\\)) main independent variable of interest (\\(X\\)) other independent variables (confounders of the effect of \\(X\\) on \\(Y\\)), that means any additional \\(Z\\)s unobserved variable (\\(U\\)) You need to think about how these concepts can be measured and what the relationships between these concepts are. 3.1.4 Develop the empirical strategy to investigate causal effects After you have specified your causal model, ask yourself how you can capture the causal relationship between \\(X\\) and \\(Y\\)? In other words, can you apply an appropriate research design that can accommodate you in determining causality with the data at hand? Use the potential outcomes framework notation to describe the types of effects you aim to identify. Following the classification by Matthay et al. (2020), that means performing, for example, a Randomized Controlled Trial (RCT): this is be the ideal world case where we randomize treatments to study effects. You actively assign a treatment to subjects by chance. Most secondary data are collected retrospectively and thus will not allow performing a RCT as you will not be able to manipulate treatment assignment. You will not be able to randomly assign subjects ahead of time, such that you typically need to search for the best available quasi-experiment to identify treatment effects using a Confounder Control Study Regression adjustment Matching Techniques Simulation Random or Fixed effects regressions Instruments Regression discontinuity design (RDD) Difference-in-difference (DiD) analysis Instrumental variable (IV) approach Any other method such as synthetic control groups, structural equation models, causal machine learning Think about what type of treatment effect you are investigating in your causal analysis. The treatment effect is the average (across the population or across some subpopulation) of the change in outcome (\\(Y\\)) that results from a change in a covariate (the treatment \\(X\\)) (Lewbel 2019). Common types of treatment effect parameters are average treatment effects (ATE) average treatment effects on the treated (ATT or ATET) marginal treatment effects (MTE) local average treatment effects (LATE) quantile treatment effects (QTE) intention to treat effect (ITT) 3.1.5 Investigate suitable source data You need to find and collect suitable data to populate your causal model. This might take some time, and not all of the variables can be found in one dataset. Frequently, different data sources need to be collected and combined. You need to decide on what aggregation level you need to conduct your analysis to estimate the cause-and-effect relationships postulated, for example, individual (patient, student), organization (hospital), regional (country, state). Given that, you need to identify appropriate data sources that contain your variables of interest at the given aggregation level. When searching for the data sources, you can turn to already existing datasets specifically designed for scientific use, for example, surveys (NAMCS, CPS, NLSY, HILDA, RLMS, KiGGS); panels (for example, GSOEP, SHARE, HRS, ELSA); censuses (e.g., Microcensus). Some data are available from statistical offices (Eurostat in the European Union, DESTATIS for Germany), private companies (health insurances), Social Media and App-based data (Mappiness, Fitbit, Facebook, Twitter, WayBetter) or governmental and non-governmental institutions (European Medicines Agency, Food and Drug Administration). Sometimes you have to hand-collect and digitize the necessary data, for example, from historical documents, data from governmental and non-governmental agencies, commercial providers and library search engines, archives; download statistical tables from INKARs or Unemployment Agencys websites. Therefore, you need to plan whether and how these can be accessed and how much time is needed for extraction. Consider automated tools like scraping. 3.2 Resources box 3.2.1 Key terminology in causal inference based on Matthay et al. (2020) Causal model: A description, most often expressed as a system of equations or a diagram, of a researchers assumptions about hypothesized known causal relationships among variables relevant to a particular research question. Treatment, exposure, or independent variable: The explanatory variable of interest in a study. Some also describe this as the right-hand-side variable. Outcome, dependent variable, or left-hand-side variable: The causal effect of interest in a research study is the impact of an exposure(s) on an outcome(s). Potential outcome: The outcome that an individual (or other unit of analysis, such as family, neighbourhood, physician) would experience if his/her treatment takes any particular value. Each individual is conceptualized as having a potential outcome for each possible treatment value. Potential outcomes are sometimes referred to as counterfactual outcomes. Exogenous versus endogenous variables: These terms are common in economics, where a variable is described as exogenous if its values are not determined by other variables in the causal model. The variable is called endogenous if it is influenced by other variables in the causal model. If a third variable influences both the exposure and outcome, this implies the exposure is endogenous. 3.2.2 Openly available textbooks and resources on econometrics with a causal inference focus Causal Inference: The Mixtape by Scott Cunningham What If by Jamie Robin and Miguel Hernan The Effect: An Introduction to Research Design and Causality by Nick Huntington-Klein How Do We Know if a Program Made a Difference? A Guide to Statistical Methods for Program Impact Evaluation by MEASURE Evaluation Open Source Economics Basics to microeconometric methods Introduction to Econometrics using R by Christoph Hanck Econometrics by Bruce E. Hansen Mastering Econometrics by Joshua Angrist Differences-in-Difference design, Health Care Policy Science Lab Statistical Tools for Causal Inference by the SKY Community 3.2.3 Ladders of causality and directed acyclic graphs A more comprehensive overview of causal paths is provided by Nick Huntington Klein A tool to draw Directed Acyclic Graphs (DAGs) is DAGitty Pearls Causal Epistemology 3.2.4 Potential outcomes framework notation Causal Inference Chapter in Health Services Research Methods I The fundamental problem of causal inference 3.3 Checklist to get started with designing your study Describe your outcomes, variables of interest, confounders and potentially, unobserved variables Identify at which rung in the ladder of causality you will be investigating your research question Get familiar with the foundations of your selected identification and estimation strategy Identify appropriate secondary data sources that populate your causal model, ensure that access is granted within the given time frame of your project Challenge your identifiability assumtions Refer back to similar empirical studies and reflect on their design and identification strategy 3.4 Example: Hellerstein (1998) The identified research question and the hypothesis that physicians are more likely to prescribe generics to patients who do not have insurance coverage for prescription pharmaceuticals can be modelled with the help of a directed acyclic graph (DAG). # set theme of all DAGs to `theme_dag()`--&gt; minimalist library(ggdag) theme_set(theme_dag()) Hellcoords &lt;- list( x = c(Y =5, P=1, C=1, X=2, R=3,XP=4), y = c(Y =0, P=0, C=-1, X=-1, R=-1,XP=-1)) Hellersteindag &lt;- dagify(Y ~ P + C + X + R + XP, P ~ X + XP + C + R, labels = c(&quot;Y&quot; = &quot;Prescription of Generic Drug&quot;, &quot;P&quot; = &quot;Patient&#39;s Insurance Status&quot;, &quot;C&quot; = &quot;Drug Prices (Class Fixed Effects)&quot;, &quot;X&quot; = &quot;Patient Characteristics&quot;, &quot;XP&quot; = &quot;Practice Characteristics&quot;, &quot;R&quot; = &quot;Region&quot;), exposure = &quot;P&quot;, outcome = &quot;Y&quot;, coords=Hellcoords) #ggdag(Hellersteindag, text = TRUE, use_labels = &quot;label&quot;) ggdag_classic(Hellersteindag) Figure 3.2: Directed Acyclic Graph, Hellerstein (1998) Figure 3.2 points out the effect of interest as well as potential confounders. Hellerstein sets up an observational, retrospective analysis of secondary data, including patient and physician characteristics. The goal of the analysis is to identify a causal effect of the patients insurance status (independent variable, \\(P\\)) on the prescription of a generic over a trade-name drug (dependent variable, \\(Y\\)). As the physicians prescription decision is prone to involve information imperfections and agency problems, the empirical model needs to control for physician-specific effects among other possible influencing factors generally represented by confounders \\(Z\\), and specifically by drug prices \\(C\\), patient characteristics \\(X\\), practice characteristics \\(XP\\) and the region \\(R\\). Another factor Hellerstein is concerned about is random variation at physician level in the choice between generic and trade-name drugs. The study is based on data from the National Ambulatory Medical Care Survey (NAMCS) 1991, provided by Centers for Disease Control and Prevention. The NAMCS is a national sample survey administered by the National Center for Health Statistics. The NAMCS data contain all actions taken by a physician in this period. NAMCS allows controlling for physician-specific behavior that is necessary to draw conclusions of our estimand, the effect of the patients insurance status. The confounder control study design chosen by Hellerstein (1998) uses a random effects probit specification at physician level to estimate the effect of insurance status on generic compared to trade-name drug choice. This includes the outcome variable (\\(Y\\)), a binary variable, which indicates whether a patient has been prescribed a trade-name or a generic drug. \\(Y\\) takes the value \\(1\\) if a generic drug or the value \\(0\\) if a trade-name drug was prescribed. Of interest is the causal effect of the patients insurance types that include Medicare, Medicaid, HMO/prepaid and private insurance. Other independent variables included in the analysis for the purpose of reducing possible bias are the drug class as proxy for prices, patient characteristics (age, gender, race), physician specialist status, the region and the averages of patient characteristics at physician level. The physician-level practice demographics help portraying heterogeneity in the physicians practice composition, as they contain all patients, including the patients that did not receive any drug prescription. Note that Hellerstein does not use the potential outcomes notation, but we assume that the variables described as confounders besides the variable of interest \\(P\\) represent all necessary confounding variables. This assumption can only be informed from theory and cannot be validated using empirical methods. Hellerstein (1998) specifies the following estimation equation: \\[P(Generic_{ij}=1|C_k,X_i,\\overline{P}_j,S_j, R_j,\\overline{X}_j,\\overline{P}_j)\\] \\[= C_k\\lambda+X_i\\beta+C_kP_i\\gamma+S_j\\pi_1+R_j\\pi_2+\\overline{X}_j\\pi_3+\\overline{P}_j\\pi_6+v_j+\\epsilon_{ij}\\] where \\(C_k\\): drug classes \\(X_i\\): vector of patient demographics \\(P_i\\): insurance categories \\(S_j\\): indicator if a physician is a specialist \\(R_j\\): region \\(\\overline{X}_j\\): practice level patient characteristics based on \\(X_i\\) \\(\\overline{P}_j\\): share of patients in each insurance category in a physicians practice Contrary to Hellerstein (1998), this equation leaves out indicators for mandatory substitution laws, \\(M\\) and \\(T\\), as this information was in confidential data files. Hellerstein describes that she uses a random effects probit specification at physician level, however, the specified equation looks different than the usual random effects specification. You can find more information on random effect models in Huntington-Klein (n.d.). The choice of the covariates depends on the underlying causal model that is rooted in theoretical considerations. Often you will not find all variables that are confounders directly. Your creativity is needed to find appropriate proxies in the data at hand to account for any relevant possible biases. An example from Hellersteins (1998) study design is the need to include prices into the empirical analysis. Hellerstein assumes that physicians are price sensitive. The physicians might not know the true price difference between trade-name and generic drug, but have an expectation of price differences. This poses a potential bias, as the physician might alter her prescription style to these expectations. Prices are not provided in NAMCS such that unobservable price differences are accounted for by including drug class dummy variables. The rationale for drug class dummies is that prices vary across drug classes. Including drug class dummy variables allows capturing variation in the prices of products indirectly. The empirical specification by Hellerstein uses a random effects probit estimator that is efficient under certain assumptions. The identifiability of the effect of insurance status on generic compared to brand-name products is not explicitly described. The specification of the estimator is described in (Cameron and Trivedi 2005, 705). Note that while random effects estimators are typically used in panel data that use a time dimension, Hellerstein uses multiple observations at drug-class level instead of time dimensions.1 References "],["run-study.html", "Chapter 4 Running the study and collecting the data 4.1 Basic steps when collecting the data 4.2 Basic steps when processing and preparing the data 4.3 Resources Box 4.4 Checklist to running the study and collecting the data 4.5 Example: Hellerstein (1998)", " Chapter 4 Running the study and collecting the data In this chapter we specify how to collect and use the data. As this study guide focuses on secondary data, you will need to identify relevant data sources, obtain the source data and process them. 4.1 Basic steps when collecting the data As many data sources are not primarily developed for the particular purpose of your study, you often will need to modify the source data to reflect the measurements of your \\(Y&#39;s\\), \\(X&#39;s\\) and \\(Z&#39;s\\) you wish to examine in your causal model. It is also often that secondary data is not readily available for analysis such that you will need to collect and process these data (for example, by writing a program that extracts data from a website, search engine, or API, or by applying for permission to access the data). We distinguish between source dataset(s) and the analysis dataset that you will be using to run the study. We point to the following distinction that will have implications on the resources needed to collect and process: Secondary source data that is readily available and has been documented earlier (for example, panels) Secondary source data that you will need to process for the purpose of your study (for example, by use of web-scraping, use of web-APIs, or extraction from data warehouses) The following steps are typically performed when collecting the data: 4.1.1 Identify secondary data sources Once you have designed the study, you can now search for appropriate data sources of your \\(X&#39;s\\), \\(Y&#39;s\\) and \\(Z&#39;s\\). That means you investigate which data items of one or multiple secondary data source(s) you are using and how these items can be collected. You have to document where, how and which data you gathered and processed. Specify exactly the variables that you will be extracting and any exclusions of the data source. If you are generating a dataset yourself from another source through scraping techniques, you also need to document how the data are collected and processed, for example, by sharing the corresponding script and a description of that data. You need to assess the suitability of the potential secondary data sources. Types and use of secondary data sources have been described across different disciplines, for example, Hair, Page, and Brunsveld (2019) for business data, Eriksson and Ibáñez (2016) for drug utilization studies, Fitchett and Heafner (2017) for social studies. It is important to assess the quality of the secondary data sources by the following criteria (Hair, Page, and Brunsveld 2019): measurement validity: This may be difficult to assess, but you may want to look for research that has used the data you aim to use reliability: Does the source that you are using provide the data of sufficient quality? potential bias This arises when the data do not measure what you want to measure. Biases may arise from, for example, changes in the sample from which the data are collected (for example, if an institution changes the way data are reported or if the sampled population is changing) 4.1.2 Develop a data collection and analysis plan Use a data scheme to describe how you combine different secondary datasets into your analysis dataset. Even if there is only one source dataset that you are using, you may need to think about how this dataset needs to be processed to be fit for your final analysis purpose. Pay attention to how different data sources are linked: are there unique identifiers of the invidual subjects that you are studying (for example, identifiers for patients, physicians, firms, products). Are they the same across the different source data? Per each dataset, describe which variables or items you plan to collect and how you aim to process the source data to fit the purpose of your study. 4.1.3 Obtain access to source data or generate the data Investigate how you can obtain access to the data including permissions, fees and any ethical considerations. Note that many data sources require registration and/or charge fees for obtaining access and processing requests. Special conditions for academics and students often apply. You may need to describe your research project and ask for permission. Allow these steps to take considerable time. 4.2 Basic steps when processing and preparing the data Once the source data has arrived, been extracted or simply you hit the download button, you need to develop a program (Stata do-file, R-code or code using other software) that documents the data extraction steps and that leads to an analysis dataset. Make sure you save all raw source data in a safe place that in terms of data protection it is ensured, that you do not accidentally delete or overwrite these data. When you develop the program, you will need to make decisions informed by the previous steps related to Combining, merging, appending datasets Specifying datasets and variables needed to estimate your regression model Which variables are included/excluded? Which variables need to be modified (recoded/calculated)? How? Creating new variables Labeling the variables Defining the study period Defining exclusion criteria (for example, teens or adults only, general practicioners or physicians) Aggregating to the appropriate level (individual, family, county, state, country, patient, physician, or hospital, etc.) Specifying the final analysis dataset: what outcomes (\\(Y\\)), variables of interest (\\(X\\)), confounders (other \\(X\\)s), instruments (\\(Z\\)) are possible to use from the data? 4.2.1 Generate the analysis dataset There are three major tasks that are typically needed to generate the analysis dataset: 1. Clean your data. get/collect the data and transfer them in a format you use, for example, to .dta from .xlsx (if neccessary) Make sure that you link different datasets correctly using correct identifiers. Take time to look through the data, check them, and delete anything that looks suspicious. Select your sample of interest. Generate and leave only variables necessary for your anaylsis. Ensure that you make plausible and relevant exclusion decisions, for example, regarding the time period studied, products, health conditions, age groups. 2. Structuring and aggregating of the analysis data Only include variables of your empirical model in the analysis dataset! Aggregate your data to the level of analysis. For example, if you aim to analyze physician behavior over time, there should be one observation for each time period and physician (panel data). If your data are purely cross-sectional, there should not be multiple time periods in your data. That means in a cross-section of physicians, one row represents one physician. 4. Store your data Your analysis dataset is the most important piece for your analysis. Physically store the version of your analysis dataset that you are using. Use version control if you are modifying your analysis dataset. Ensure that the code to create your analysis dataset is complete and can reproduce the analysis dataset completely. 4.3 Resources Box 4.3.1 How to organize your research project: Folder structure Tips, tricks and software for keeping research organized Organizing a research project 4.3.2 Coding practices: Best Practices for Data and Code Management in Projects General principles on How to code well Make program files self-contained Use relative paths Identify inputs and outputs Automize Be consistent Comment and document Use spacing and indentation Do not substitue brevity for readability Beware of error causing codes (small caps, commas, semicolon) 4.3.3 Data cleaning LOST Library of Statistical Technqiues - Data Manipulation Section Cleaning data in STATA 4.3.4 Collecting data from meta-data and web-scraping Introduction to web scraping: Resources 4.4 Checklist to running the study and collecting the data Develop a data extraction and analysis plan by describing how your final analysis data set should look like Set up data extraction methods if your data is not readily available to use, for example a scraping algorithm Go back to your causal model and identification strategy: Does the analysis data allow the analysis you like to perform? Most importantly, is it collased at the level needed? Set up a reproducible program code of all steps that you take to clean, combine and aggregate the data Store and document your analysis data set in a codebook 4.5 Example: Hellerstein (1998) The empirical application of Hellerstein (1998) is based on three datasets as provided by the National Ambulatory Care Survey (NAMCS) from the year 1989: NAMCS, containing demographic and medical information on the full sample of patients NAMCSd, containing information on medications prescribed or given to a subsample of patients of the NAMCS data. NAMCS confidential data, key to identifying physicians state of origin The confidential data are not publicly available but are key to identifying physicians and patients. Thus, for the purpose of reproducing Hellersteins results, we resort to the publicly available data of 1991, that provide these identifiers in two publicly available datasets: NAMCS, containing demographic and medical information on the full sample of patients (documentation can be found here). NAMCSd, containing information on medications prescribed or given to a subsample of patients of the NAMCS data (documentation can be found here). The NAMCS contains data on patient visits to non-federally employed physicians in the United States. For each visit, the physician or other staff members have to complete a one-page survey form containing inter alia patient demographics including the patients insurance status, diagnoses, types of health behavior counselling, medications ordered or provided, as well as provider characteristics. As the physicians are randomly chosen and randomly assigned to a two-week reporting period, the NAMCS is able to create a nationally representative sample. The publicly available data of NAMCS and NAMCSd come in an .exe format and can be downloaded at the Centers for Disease Control and Prevention (CDC) website. We use the unpacking software 7-zip to convert the files in a .txt format. The NAMCS dataset contains demographic and medical information on the full sample of patients and is mainly used to introduce the sample. The NAMCSd is a subsample of patients of the NAMCS data. It is the sample used for all estimations later on. In it, each observation covers one medication, that was mentioned, prescribed or given to a patient, and comes along with further information on both the medication and the patient receiving it. Usually, certain transformations need to be performed to be able to use the raw dataset. This can apply to the file or data format, the structure of the dataset or you might need to combine different datasets. After downloading the raw data from the CDC website, a challenge is to structure the raw dataset to be able to use the relevant variables in Stata. After importing the text file, all data were condensed to one string of characters in a single column. This means that we first need to indicate the different data items (variables) for the individual information (i.e. patient characteristics). Therefore, to be able to use the data, we need to manually split the string into the desired format, so the information is split in separate variables. Which digits in the initial string belong to one data item is defined in the documentation files (also often referred to as a codebook) provided by the CDC website. Almost every dataset comes with a codebook, containing useful information on the coding or handling of the data. This can often help and provide the necessary information needed to successfully prepare your data for empirical analysis. For the reproduction of Hellerstein (1998) we use Stata, creating a do-file containing the code (Section 8.1). It is important to develop a program file of your code containing every step you performed to manipulate and analyze your data. This ensures the reproducibility of your research and enables you to go back to earlier versions of your data preparation and analysis, if necessary. Before going into further detail on the data preparation, we want to emphasize on the importance of properly setting up your working directory. In a single project you are likely to save, use, override and delete a multitude of files. Without using a strict system of organizing your files and folders, you will inevitably run into trouble. There is no optimal structure, and you must find what works best for you, but in general, contents of folders and subfolders should be as homogenous as possible. To make this less abstract, let us demonstrate how we manage our working directory on the data-related part of this reproduction paper. The Stata code is the heart of your empirical analysis. To make it as comprehensible as possible, we divided the code into four parts. The first part translates the raw data into a readable format, the second one applies Hellersteins preparatory processes, the third reproduces the descriptive statistics and the last contains the code to run the empirical analysis. Corresponding to these do-files, we created sub-folders that contain required data or offer a place to store the output. By saving the paths to all your relevant subfolders as global, you can easily navigate through your working directory. Setting up your working directory at the start of a project will also help you to keep an overview on what is done and what is to come. You can find an example of the pathing and setup in (Section 8.1 line 12 to 21). Once the NAMCS and NAMCSd are turned into readable formats we perform the following cleaning (i.e. exclusion of certain observations), transforming and preparation steps, that are needed to reproduce the tables and figures originally produced by Hellerstein (1998): For the NAMCS, used exclusively for the reproduction of table 1 (Section 8.1): Missing data must be relabelled according to the syntax of the statistical software and observations with missing data were not considered. Keep only those observations that uniquely identify a single source of payment, meaning to drop observations with (A) missing insurance status, or (B) multiple insurance statuses. Step (B) is not mentioned in the text but can be inferred by the values of Table 1. Despite some observations reporting multiple sources of payment, the means of the mentioned payment/insurance options aggregate to 1. Thus, Hellerstein must have treated these observations as invalid. The setup of the dummy variables for Medicare and Specialists underlie special conditions (details can be found in the do-file attached), whereas the remaining dummies were not created straight away. For the NAMCSd, used for all other figures and tables, Clear missing values and perform steps (A) and (B) as in the NAMCS data. Although reported in the table 1, drop observations that report other government insurance as a source of payment. Keep only those observations of drugs that are the first mentioned drug in a patient visit. Keep only those observations of drug mentions, that are part of the eight largest drug classes. The names of the included drug classes can be found in table 3 and the labels the data use in the documentation files. Keep only those observations of drugs that were prescribed. Create the same dummies as in the preparation of the NAMCS data (footnote table 2, ??) Create an indicator on whether a drug is a multisource drug (definition given in ch.1 paragr. 1: We categorized drugs with the same ingredients and define drugs in an ingredient-group as multisource if there is at least one generic and one tradename drug within a group.). Drop observations of medications that are not multisource drugs. Use the variable on whether a drug is a generic or trade-name drug to create an indicator on whether the drug is a generic (dependent variable). Create physician averages of variables listed on p. 123 in Hellerstein (1998). After completion of these steps, the analysis dataset NAMCS includes 43 variables and 33,123 observations and NAMCSd 29 variables and 8,397 observations. The dataset is aggregated on patient level and is now ready for statistical analysis (see at the end of the next chapter). Of importance is to ensure that the raw data, the data preparation do-file and the analysis dataset are stored safely. The NAMCSd dataset serves as a source for the empirical analysis of Hellerstein (1998). Table (#tab:analysisdat): Variables of the analysis dataset based on NAMCSd Variable Name Label generic_status 1 if patient receives generic drug, 0 if trade-name ingredients Ingredients of the drug prescribed drug_class 8 largest drug classes prescribed age Patient age hmo_pre_paid HMO insurance medicare Medicare insurance medicaid Medicaid insurance other_gov_ins Other government insurance private_ins Private insurance selfpay No insurance other_pay Other payment physician_id Physican individual indicator patient_id Patient individual indicator female 1 if patient is female, 0 if male nonwhite 1 if patient is nonwhite, 0 if otherwise hispanic 1 if patient is hispanic, 0 if otherwise northeast 1 if practice setting is in the northeast, 0 if otherwise midwest 1 if practice setting is in the midwest, 0 if otherwise south 1 if practice setting is in the south, 0 if otherwise west 1 if practice setting is in the west, 0 if otherwise specialist 1 if physician is a specialist, 0 if otherwise mean_age Mean age of patients in an individual practice mean_female Mean percentage of females in an individual practice mean_nonwhite Mean percentage of nonwhites in an individual practice mean_hispanic Mean percentage of hispanics in an individual practice mean_medicare Mean percentage of patients with medicare insurance in an individual practice mean_medicaid Mean percentage of patients with medicaid insurance in an individual practice mean_hmo_pre_paid Mean percentage of patients with HMO insurance in an individual practice mean_private_ins Mean percentage of patients with private insurance in an individual practice Table 4.1 shows the variables included in the dataset. References "],["analyzing.html", "Chapter 5 Analyzing the Data 5.1 Basic steps when analyzing the data 5.2 Resources box 5.3 Checklist to analyzing the data 5.4 Example: Hellerstein (1998)", " Chapter 5 Analyzing the Data 5.1 Basic steps when analyzing the data Think again about your research question and what you are trying to learn or discover. How can you use your data to answer this question? The data to be analyzed should correspond to the core elements of the hypothesis to be investigated. You investigate whether the postulated cause-effect relationship exists, or quantitatively identify the strength of the effect. To perform a reproducible research project, you need to generate an analysis dataset. For information on how to generate an analysis dataset and how to create a set of programs that describes how the secondary data used were synthesized, processed and analyzed, go to Basic steps when processing and preparing the data. Using your analysis dataset you perfrom two more steps to examine your research question. Across both steps, try to use data visualization with tables and graphs. They are an essential part of your work and not an accessory. Visualizations help to support the argumentation in the text and visualize complex facts in a simple form. In the following, we describe the two major tasks for data analysis Describe your data, assess validity and plausiblity Generate estimates of your investigated effect using regression techniques 5.1.1 Describe your data, assess validity and plausiblity Start the data analysis by doing some descriptive analyses of your data and sample. Once you have selected the necessary variables, generated new variables for the analysis, and combined all the necessary datasets into one analysis dataset you can start your empirical investigation. Check the plausibility by looking at basic descriptives (N, mean, median, frequencies) Plot the distribution of your data using histograms, boxplots, bar charts Critically reflect any anormalities: Compare your data with the reference literature Are descriptives similar? If not, why so? Try to assess coding problems, different population, unbalanced samples. 5.1.2 Generate estimates of your investigated effect using regression techniques Once you have decided on the type of empirical analyses you would like to perform, run the regressions. Perform the regression analyses. That means use the appropriate procedure to estimate regressions that reflect your empirical strategy. Create output tables that report your results which outsiders can understand. Label your variables properly Concentrate on analyzing and interpreting the effects of your variable of interest (\\(X\\)) interpret and show in tables only most important and relevant coefficients related to the research question. For example, you do not need to interpret and display coefficients for all of the incuded control variables, just the main variables of interest. Think about how to interpret your estimates. Challenge your approach. Investigate why your estimates could not be plausible? Again, compare with existing literature. What do your data say? After you run the regressions, look at the results: Are your main coefficients of interest statistically significant: is the p-value smaller than 0.05 or any other pre-defined level of significance? Consider also the economic significance of your results. Is the effect you are measuring large or small? Do your results make sense or are they counterintuitive? This might give you a clue that you might have misspecified your regression or made a mistake in your analysis. 5.2 Resources box Repsitories of statistical techniques, including data wrangling LOST - Library of Statistical Techniques Data Analysis Princeton University Library: Getting Started in Data Analysis using Stata and R Data Analysis for Business, Economics, and Policy Organizing your workflow, programming and automation Gentzkow M, Shapiro J. Code and Data for the Social Sciences: A Practitioners Guide [Internet]. Chicago Booth and NBER; 2014 The Stata workflow Guide In Stata coding, Style is the Essential: A brief commentary on do-file style Data Visualization The chapter Data Visualization Basics by Hans Sievertsen includes important resources for the fundamentals of data visualization Stata Cheat Sheet on Visualization provides an overview of the technical implementation Jones AM. Data visualization and health econometrics. Foundations and Trends in Econometrics. 2017 Create journal submission-ready output tables Creating Publication-Quality Tables in Stata Stata commands to plot regression coefficients, make regression tables and visualize results by Ben Jann 5.3 Checklist to analyzing the data Follow your research question and hypothesis. Do not alter it while analyzing the data. Do your results make sense? If not, rule out any programming errors? Try to interpret your data? How big is your effect? Is it small or large? Use visualizatoin techniques as often as possible to analyse your data. Discuss your results with 5.4 Example: Hellerstein (1998) Hellersteins 1998 study aims to identify the role the physician plays in prescribing a generic over a trade-name drug and the role of a patients insurance status. To identify this, we analyze the secondary data on physician level in cross-sectional format (NAMCS and NAMCSd, described in the chapter running the study and collecting the data). The primary interest is to investigate if physicians do or do not prescribe a generic drug and whether this is influenced by the patients insurance status. We are not investigating prescription behavior of physicians over time using panel data. Over the course of the analysis, we want to explore the dataset. This includes describing and assessing the validity and plausibility. Thus, before generating estimates of the effect of interest using regression techniques, we describe the data using descriptive statistics presented in tables and figures. We first reproduce the descriptive statistics of Hellerstein (1998), namely tables 1-3 and figures 1-2. Details on created variables can be found in the data preparation do-file (Sections 8.1-8.3) and the notes to the tables and figures in Hellersteins 1998 paper. The code for the reproduction of the descriptive statistics (i.e. the reproduction of tables and figures), can be found in (Section 8.2. Please keep in mind that we use the data from 1991. Thus, our numbers are not matching with the numbers provided in the original paper. 5.4.1 Describe your data, assess validity and plausiblity 5.4.1.1 Table 1 We compute the mean and summary statistics for the age and the previously established dummies in the NAMCS data (Section 8.2). Hellerstein Table 1 - Summary Statistics for Overall NAMCS Patient Sample Variable mean sd Age 43.07 24.81 Female 0.59 0.49 Nonwhite 0.11 0.31 Hispanic 0.06 0.23 Self-pay 0.22 0.41 Medicare 0.14 0.35 Medicaid 0.10 0.30 Private/Commercial 0.37 0.48 Other government insurance 0.02 0.15 HMO/prepaid plan 0.15 0.36 Specialist 0.68 0.46 Northeast 0.23 0.42 Midwest 0.25 0.44 South 0.28 0.45 West 0.24 0.43 Notes: Data source: NAMSC91. Sample size is 29,854. Sample size differs as we use data from a different year. However, the public data from 1989 (the one Hellerstein uses) allow for reproduction of Table 1. For further information see Table 1 notes in Hellerstein (1998); With the specifications stated in the manuscript, we cannot reproduce table 1 completely. 5.4.1.2 Table 2 For the first two columns, we repeat the procedure of Table 1 using the NAMCSd data. For the third column, we compute the mean of the generic indicator for all subpopulations defined by the dummies (Section 8.2). Hellerstein Table 2 - Summary Statistics for Patients in NAMCS Drug Sample Mean Standard Deviation Proportion Generic Age 43.79 25.13 Female 0.59 0.49 0.27 Nonwhite 0.12 0.32 0.34 Hispanic 0.06 0.24 0.33 Self-Pay 0.27 0.44 0.29 Medicare 0.15 0.36 0.21 Medicaid 0.11 0.31 0.32 Private/Commercial 0.33 0.47 0.27 HMO/prepaid plan 0.15 0.35 0.34 Specialist 0.60 0.49 0.26 Northeast 0.21 0.41 0.28 Midwest 0.27 0.44 0.27 South 0.29 0.46 0.25 West 0.23 0.42 0.35 Full sample 0.28 Notes: The sample size is 7,715. For further notes see Hellerstein (1998), Data source: NAMSCd91. 5.4.1.3 Table 3 Using the summary statistics command, we count the number of observations for the full sample and for each of the eight defined drug classes. We compute the mean of the generic indicator in the full sample and each drug class subsample (Section 8.2). Hellerstein Table 3 - Frequency of Generic Prescription by Drug Class Observations % Generics All drugs 7715 28.37 By drug class Antimicrobials 2955 40.37 Cardiovascular-renals 1344 16.15 Central Nervous System 789 25.48 Hormones/Hormonal mechanisms 917 35.66 Skin/Mucous membrane 530 9.06 Ophthalmics 295 13.90 Pain relief 634 21.29 Respiratory tract 251 10.76 5.4.1.4 Figure 1 For each physician, we compute the mean of the generic indicator and drop duplicate observations per physician. This way, every physician has a unique observation with a respective share of generic prescriptions (Section 8.2). To approximate the distribution shown in Figure 1, we chose the following specifications based on visual approximation like: The decimal places at which the average generic share was rounded: \\(.01\\) The bandwidth: \\(0.02\\) The kernel function: bilinear form Figure 5.1: Hellerstein Figure 1 - Distribution of Physician Generic Prescription Rates (Source: NAMSCd91) 5.4.1.5 Figure 2 We only keep physicians that prescribe the same multisource drug to at least six patients, independent of whether it is in its generic or trade-name form. We compute the mean of the generic share for each remaining physician and create a categorical variable on whether this mean is 0, indicating only trade-name drugs; 1, indicating a mix of generic and brand-name versions; or 2, indicating only generic drugs. We plot the frequencies of these three categories across physicians in a bar plot (Section 8.2). Figure 5.2: Hellerstein Figure 2 - Physician decisions by physicians who prescribe a drug to at least six patients (Source: NAMSCd91) 5.4.2 Generate estimates of your investigated effect using regression techniques Hellerstein applies a random effects probit regression model aggregated on physician level. The parameterization of the model is implemented as follows: Dependent variable (\\(Y\\)) \\(G\\): Generic compared to brand-name drug use Note that Hellerstein uses a somewhat different notation for the outcome variable which is denoted as \\(G\\). Today, it is often common to denote this variable by \\(Y\\). Variable of interest or treatment (\\(X\\)) \\(P\\): Insurance status by: Medicare, Medicaid, HMO/prepaid, private insurance, self-paid (this is the omitted category). Note that Hellerstein (1998) uses a somewhat different notation for the variable of interest which is denoted as \\(P\\). Today, it is often common to denote this variable by \\(X\\) or \\(D\\). Though for the purpose of the reproduction and to avoid confusion we take on Hellersteins notation in the following. Confounders \\(Z\\) of the effect of insurance status \\(P\\) and generic vs brand-name drug use \\(Y\\). \\(C\\): Drug class identifiers among 8 classes (Pain relief (omitted), see footnote Table 5) \\(X\\): Patient characteristics: age, sex, race \\(S\\): Physician specialist status: Specialist, general practitioner (omitted) \\(R\\): Region as classified by: Midwest, South, West, Northeast (omitted) Vector \\(X\\) Average patient characteristics in one practice Vector \\(P\\): Average of a physicians patients in each insurance category Note that Hellerstein uses a somewhat different notation for confounding variables. It is often common to denote the variable of interest using the index \\(X\\), but not necessarily confounders. 5.4.2.1 Tables 4 and 5 We estimate the regression coefficients, t-statistics and marginal effects of the regression model. It is not indicated whether the model for both tables contain all covariates specified in equation 10 or just the variables mentioned in the tables. We decide to use the full model for both tables. Marginal effects are the average marginal effects across individuals (footnote Table 4) (Section 8.3). Hellerstein Table 4 - Estimated Coefficients on Demographic Variables, Geographic Variables, and Average Characteristics for Full Sample, excluding regional identifiers Random-Effects Probit Coefficient % Change in Generic Constant -0.556** (-3.17) Age -0.002* (-1.99) -0.001* (-1.99) Female -0.112** (-2.94) -0.028** (-2.94) Hispanic 0.025 (0.28) 0.006 (0.28) Nonwhite 0.044 (0.62) 0.011 (0.62) Specialist 0.019 (0.25) 0.005 (0.25) Mean age -0.003 (-1.10) -0.001 (-1.10) Percent female -0.189 (-1.16) -0.048 (-1.16) Percent black 0.075 (0.41) 0.019 (0.41) Percent Hispanic -0.112 (-0.45) -0.028 (-0.45) Percent Medicaid 0.062 (0.26) 0.016 (0.26) Percent Medicare -0.155 (-0.71) -0.039 (-0.71) Percent private insured 0.155 (1.08) 0.039 (1.08) Percent HMO/prepaid 0.054 (0.30) 0.014 (0.30) Midwest -0.141 (-1.53) -0.036 (-1.53) South -0.206* (-2.28) -0.052* (-2.29) West 0.065 (0.67) 0.016 (0.67) Notes: \\(*p &lt; 0.05, **p &lt; 0.01, ***p &lt; 0.001\\); The sample size is 7,715. For further notes see Hellerstein (1998); Data source: NAMSCd91. Table 5 shows the estimated coefficients for the eight largest drug classes as well as the % change in the generic share. The greatest change compared to Hellerstein (1998) is the change in sign from positive to negative for cardiovascular/renals, though not significant in our model. Hellerstein Table 5 - Estimated Coefficients for Drug-Class Dummy Variable for Full Sample Random-Effects Probit Coefficient % Change in Generic main Antimicrobials 0.628*** (8.47) 0.159*** (8.54) Cardiovascular-renals -0.147 (-1.71) -0.0371 (-1.71) Central Nervous System 0.0559 (0.58) 0.0141 (0.58) Hormones/Hormonal mechanisms 0.584*** (6.92) 0.148*** (6.95) Skin/Mucous membrane -0.731*** (-6.26) -0.185*** (-6.28) Ophthalmics -0.383* (-2.47) -0.0968* (-2.47) Resperatory tract -0.675*** (-4.79) -0.171*** (-4.80) Notes: \\(*p &lt; 0.05, **p &lt; 0.01, ***p &lt; 0.001\\); The sample size is 7,715. Omitted category is pain relief. For further notes see Hellerstein (1998); Data source: NAMSCd91. 5.4.2.2 Table 6 We run a separate regression using only observations of a single drug class and report the coefficients of the payment dummies and their average marginal effects (Section 8.2). Hellerstein Table 6 - Tests for Moral Hazard for the Full Sample Equality of Individual Insurance Variables with Self-Payment Random-Effects Probit Results Antimicrobial % change Cardiovasculars % change Central Nervous System % change Hormones % change Skin/Muchous Membranes % change Ophthalmics % change Pain relief % change Respiratory Tract % change Medicaid Coefficient 0.14 0.04 0.00 0.00 0.02 0.00 0.38 0.08 0.03 0.00 -0.25 -0.05 0.54 0.08 -1.21 -0.09 1.12 1.13 0.02 0.02 0.08 0.08 1.23 1.23 0.04 0.04 -0.60 -0.60 1.12 1.15 -1.08 -1.11 Medicare Coefficient 0.11 0.03 0.05 0.01 -0.32 -0.06 -0.42 -0.09 0.54 0.05 -0.41 -0.08 0.08 0.01 -0.60 -0.05 0.72 0.72 0.25 0.25 -1.13 -1.12 -1.68 -1.69 0.89 0.90 -0.97 -0.97 0.18 0.18 -0.67 -0.68 Private Coefficient 0.00 0.00 -0.06 -0.01 -0.06 -0.01 -0.16 -0.03 0.03 0.00 -0.31 -0.06 0.18 0.03 -1.27 -0.10 0.01 0.01 -0.30 -0.30 -0.28 -0.28 -0.75 -0.75 0.07 0.07 -0.91 -0.91 0.51 0.51 -1.45 -1.56 HMO/Prepaid Coefficient 0.22 0.06 -0.14 -0.03 0.32 0.06 -0.33 -0.07 -0.42 -0.04 -0.57 -0.11 0.28 0.04 -0.26 -0.02 1.78 1.78 -0.56 -0.56 1.04 1.05 -1.09 -1.09 -0.73 -0.73 -0.93 -0.93 0.71 0.72 -0.26 -0.25 Notes: see Hellerstein (1998); Data source: NAMSCd91. 5.4.2.3 Tables 7, 8 and 9 These tables cannot be reproduced as the publicly available data of NACMSd do not provide variables to identify physicians and patients by region. "],["report-study.html", "Chapter 6 Reporting the Study 6.1 Basic steps to reporting the study 6.2 Resources box 6.3 Checklist to reporting the study 6.4 Example: Hellerstein (1998)", " Chapter 6 Reporting the Study 6.1 Basic steps to reporting the study Figure out the one central and novel contribution of your paper. Write this down in one paragraph. (Cochrane 2005, 1) At this stage you need to report all your performed work in detail in a scientific research paper. When reporting the study you should provide readers with the necessary information to reproduce every table and figure; to assess the appropriateness of methods used, given your settings and data, and its contribution and conclusions, given your results. A research paper usually includes the following sections: Title Abstract Introduction Previous Literature Background (optional) Data and Variables Empirical Methods Results Discussion Summary and Conclusion References Appendices We briefly summarize each of these items, based on Cochrane (2005), Dudenhefer (2014), Martin, 2018, and Plamen (2020)): Figure 6.1: How to report your paper Source: https://towardsdatascience.com/how-to-keep-your-research-projects-organized-part-1-folder-structure-10bd56034d3a The Title should be short (15 words or less) and reflect your research question. It usually contains the purpose (what is the question you are addressing?) and scope (if you cover a specific time period or population) of the study. You might also include the methods of your study in the title, if, e.g., you conduct a cohort study. All this information should help readers to decide whether or not they are interested in reading the paper. The Abstract is usually around 150 words or less and includes a very short summary of your research question, main contribution, data and methods used, main findings, and conclusions. The Introduction is the most important part of the paper as it has to grab the readers attention. It should motivate your research topic and give a more extensive overview of the context, data, methods and findings. You provide context by referencing the existing literature regarding what we know and what we do not know about the topic. You parlay the latter part into your research question. An important aspect of introduction is to highlight what contribution your paper makes, and how it relates to and improves previous work on the topic. You need to raise the interest of your readers to a degree that they would want to continue reading the rest of your paper. Therefore, it is advisable that you write this section at the very end. It should be structured as a newspaper article  where you first deliver the main message and information, and then provide details  rather than a novel-  where you first build up the narrative and only reach the story punchline at the very end. At the end of this section you could write a roadmap that briefly guides the reader through your paper. Overall, the whole section should not take up more than 3 pages. Once you choose your topic and pose a research question, the first thing you usually do is look at the related previous literature. The Previous literature review is a standard in economic papers and constitutes a significant part of your empirical project. It is either included in the Introduction or is placed in a separate section. The main purpose of the review is to show the reader that you are familiar with the main scholarly work in the field; to critically review the most relevant and influential (well-cited) studies on the topic; to place your study in the context of other studies; and to convince the reader of the need to conduct the research in question, and to show how it fits into and contributes to the study field (Are you using new data or identification strategy? Are you answering a question more specifically? How are you improving on previous papers?). It helps you to establish whether the topic has been researched before, to identify limitations of previous research and still open research gaps. You do not need to mention every published paper on the topic, but you do need to cite the most important studies in the area and distinguish your paper from them. Literature reviews are selective: you only include papers that are relevant for your argument. Importantly, a literature review should not simply consist of a series of paragraphs that summarize papers in no particular order. Instead, a good iterature review should be carefully constructed to tell a specific story: this is what others have done on my topic; this is what is unsatisfactory or missing from that research; and here is how I am going to fill this researh gap. Thus, through your literature review you sell the value addded of your paper. In a literature review, you need to identify and summarize key scholarly publications from the fields that are pertinent to your research topic. It should involve a thorough search of the main key words in the relevant databases and journals. To organize your literature search, you can create a spreadsheet, where you include study and author(s) names, publication journal and year, identification strategy, main resuts, caveats and comments. Always take notes on the papers you read. This will help you to summarize and organize your review once you are ready to write it. Sometimes a paper might contain a Background section that provides background information about a historical event or context, a specific countrys health care system, or details about a specific policy or program in question. The Data and Variables section should contain a detailed information about the data sources used in the paper as well as information about the variables used in the analysis. Identify your data sources and describe them, i.e. data-generating process, the unit and the number of observations, the population groups sampled, the time period during which the data were collected, the method of data collection, type of data (cross-sectional, panel, time series), main variables for your reseaarch question, etc. Mention any data limitations that might influence your results, i.e. small number of observations, survey response, missing observations, attrition or selection bias, over or underrepresentation of certain populations, etc. Describe your sample and variable selection in great detail: how you construct your sample and which data manupulations you perform. Provide descriptive statistics and graphs. Descriptive statistics usually includes means and standard deviations of variables you use in your analysis. Sometimes you need to present this statistics for different subgroups, e.g., treatment and control, post and pre treatment. Point here to the patterns that are easily observable from the summary statistics that can help the reader better understand your analysis. Generally, a good way to learn how to write a data section is to look in the related literature and pay attention to the information it includes. The Empirical method section should in great detail describe the statistical method used in the paper and the assumptions that are necessary to be fulfilled in order to get an unbiased and consistent estimation of your parameter of interest. Write whether you weight the data or use certain fixed effects. Also describe the empirical problems and how you address them. Ideally, you should describe each step that readers should perform, if they would like to reproduce your results and conclusions. You need to explain the appropriatness of the method you choose for the specific context. You need to write down your econometric equation and explain how variables are defined and measured. Mention key parameters of interest. The Results section should contain a set of tables and figures that show your main empirical results. Make sure that tables are reader-friendly and are easy to comprehend. Place figures and tables as close as possible to the place in the text where you first refer to them. Do not report all the variables incuded in the regression, but only present the results and parameters that directly relate to your main hypothethis and story. Including too namy parameters and results can distract the readers from your main point. Thus, guide the readers and focus their attention only on the most important results from your anaylsis and in the right order. Do not provide any interpretations or speculations here. Keep it factual and simply describe in words what the tables and figures display. Comment on the economic significance of your results, not just their statistical significance. After the main results, discuss potential biases and shortcoming to the internal validity and suggest ways to check them. Place secondary results and your response to potential criticisms and robustness checks into your Appendix. The Discussion section contains the interpretation and discussion of the results, as well as potential mechanisms, and suggests a direction for future research. Start your discussion by reiterating your research question and, based on your findings, state what you think an answer to your question is. Do not introduce any new data or results in this section. It should reflect only the results already presented in the paper. Interpret your results in the context of the literature that you identify and discuss in the Introduction and the Previous Literature sections (Are your findings consistent with what the other literature has found? Do your data fill the gap in the knowledge that you identify?). Here you want to show how your work adds to and extends the knowledge on the topic. You can also discuss implications of your study for public policy. You also should acknowledge and discuss the limitations of the study here and what implications these limitations might have for the results. The more accurate, open and detailed you are about the limitations, the more credibility your results will have. Suggest alternative interpretations and other possible mechanisms that can explain your findings. You can also talk about next steps and the implications of your findings for future research. The Summary and Conclusion section should be short and sweet (a few concise paragraphs). It contains the most important information on the topic, findings and concluding remarks. It should mirror main points mentioned in the Introduction. You should underline why your research matters and state the answer to your research question. Also state here any recommendations that can be made based on your findings. At the very end of your paper you need to provide a complete list of References. The bibliography includes a compete list of academic papers and books referenced and cited in your study. Make sure that all your in-text citations appear in the Reference list at the end of the paper. The easiest way to keep track of your reference list is to write down the information, i.e. the title, author(s), journal/book, publisher, and publication date, about the original source each time you use it. Decide in advance on one citation and reference style and follow it consistently throughout the paper. The Appendices are a handy tool that allows you to save space. As a careful researcher you will conduct a series of robustness checks of your main results. However, once you confirm that the results are stable, to keep your paper focused and not too long you can move all the still relevant, but secondary and less critical information in appendices. You should summarize what you did in the main body of your paper and include an in-text reference to the information in the appendix. Most journals provide possibilities of an online appendix or supplementary material. (#fig:paper_structure)Paper Structure Source: Fresno State Graduate Writing Studio. Elements of a Research Paper. 6.1.1 The writing process, tips based on Cochrane (2005) and Dudenhefer (2014) Producing a good empirical paper takes time. You should start working on it and drafting it as soon as possible. Allow yourself sufficient time for collecting and analyzing the data, writing and revising the paper. Writing a paper is a recursive process that takes many revisions. Read the text yourself and let others read it. (Is the text interesting to read?, Would you want to read beyond the introduction yourself?) Shorter is better: keep the paper short as possible. Every word must count. When editing always ask yourself whether you can make the same point with fewer words. Avoid repetition. It uses extra space and tests readers patience. If it is possible to cut a word, sentence or paragraph, then cut them out. Keep track of your resources as you work on your ideas. Do not rely on your memory when you come across information you might use in your paper. Invest in acurate note-keeping from the start. It will save you time and extra work when you draft your paper. Pay attention to the writing style in the papers you read and admire; carefully analyze what information they contain and how they put it together. Remember, no one can learn to write simply by reading a few manuals and tips on the topic. You can only do it by practicing. More principles on writing in economics are summarized in Top Ten Rules of Economical Writing and more comprehensively in McCloskey and Ziliak (2019) and by Hall (2013). 6.1.2 General guidance on writing and use of language in a research paper These tips are a collection of Cochrane (2005), Lund University, and Plamen (2020). Do not use slang, jargon, or colloquialisms. Always use formal language. Do not use common vocabulary, such as have got, the other thing. Make more formal vocabulary choices, e.g., have found, the other issue/problem/notion/idea etc.. Do not use shortened verb forms, such as theyre, isnt, cant, dont. Use the full verb form instead, e.g., they are, is not, cannot. Do not use subjective language, i.e. I think, we assume. Use objective language. Avoid using past or future tenses. Use present tense and generally stick to one tense: This paper attempts to, Hellerstein (1998) finds that; Table 1 shows Do not use passive voice, i.e. It has been shown in the paper by Hellerstein (1998). Use active voice, i.e. Hellerstein (1998) shows Do not use needless words and information that clutter the writing. Use simple and direct sentences, and keep them short. For example, change in order to to to, whether or not to whether, is equal to to equals, it should be noted to notably. Place minor or secondary details and digressions from the main text into footnotes, but use them sparingly. Use I in a single-author project, and we in the project with more than one author. Do not plagiarize. Provide references and citations whenever you say something that is not your own. Proper referencing and citations prevent acusations of plagiarisms, allow to acknowledge your sources, give credibility to your work, and demostrate your knowledge in the subject area. Use in-text citations: Hellerstein (1998) finds that Do not write Judith Hellerstein in 1998 journal article finds that Do not mix British and American English, citation styles, tenses, and different word spellings (e.g., health care vs healthcare, data set vs dataset, policy makers vs policymakers, socio-economic vs socioeconomic, etc.). Be consistent in the use of language and style. Remember that long-run as an adjective uses hyphen, while long run as a noun does not. Use automatic spellcheck, but never rely on it completely. Always double check for tricky words and proofread the text. Ask others to read it. Do not get complacent after finishing the first draft of the manuscript. Revise, rewrite, polish, carefully proofread and spellcheck. Control the word count. Read a copy of Strunk, W. Jr &amp; White E. B. (1972). The Elements of Style and Zinsser, W. (2001). On Writing Well. 6.1.3 Tables, tips based on Cochrane (2005) Each table should be self-explanatory. Give your variables in the table self-explanatory names. Use 2 to 4 digits after the comma when reporting the results, not all the numbers produced by the statistical program. Apply this rule consistently for all the tables in your paper. Show results with and without controls. For example, start with a column that includes only the main coeffitient(s) of interest and then progressively inlcude various controls (e.g., patient characteristics, hospital/physician fixed effects, etc.) in subsequent columns. Include standard errors together with your coefficients. Indicate the source of the data at the bottom. 6.1.4 Figures, tips based on Cochrane (2005) Use figures to show patterns in the data  they demonstrate it better than tables with numbers, Label the axes properly, Add clear legends, Do not color your figures since some readers will view/print your paper in the black-and-white mode, Provide self-explanatory captions, Indicate the source of the data at the bottom. 6.2 Resources box 6.2.1 Collections of writing tips and short articles Dudenhefer, P. (2014). A guide to Writing in Economics. EcoTeach Center and Department of Economics, Duke University. Plamen, N. (2020). Writing tips for economics research papers Cochrane, J. H. (2005). Writing tips for Ph.D. students. Chicago, IL: University of Chicago. Top Ten Rules of Economical Writing 6.2.2 Books Hall, G. M. (Ed.). (2013). How to write a paper (5th ed). Wiley-Blackwell. McCloskey, D. N., &amp; Ziliak, S. T. (2019). Economical Writing, Third Edition: Thirty-Five Rules for Clear and Persuasive Prose (Third Edition). University of Chicago Press. Strunk, W. Jr &amp; White E. B. (1972). The Elements of Style - great general writing style book Zinsser, W. (2001). On Writing Well 6.2.3 Videos Martin, G. (2018). How to write a paper. 6.2.4 Online resource Lund University. Academic Writing in English. 6.3 Checklist to reporting the study Start writing early. Edit your text a lot to improve it. Make sure you follow your one central research question. Ensure you cover and include all relevant sections of a research paper. Concentrate on the most important tables and figures for your main text. Plan ahead for proof-reading and editing the text you have written. Be able you read your own paper with some time distance. 6.4 Example: Hellerstein (1998) We go over each section of the manuscript to highlight its role to Hellersteins research article. The structure contains the following parts: Title Abstract Introduction Background and related literature Medical Legal Institutional Empirical motivation and summary statistics The model Empirical implementation Estimation results Conclusion Appendices References 6.4.1 Title The paper title consists of 12 words that reflect the papers research question. 6.4.2 Abstract In the abstract Hellerstein gives a short overview on the paper by mentioning the topic, the data, the results and their takeaway. 6.4.3 Introduction The introduction to the study is short. Hellerstein starts it by highlighting a welfare-enhancing potential of substituting trade-name drugs with generics, creating relevance for her topic. Next, she elaborates her research question, followed by a short overview of the results and their implications. She finalizes with a structural overview of the rest of her paper. Other common features of an introduction are missing here. These include an overview of the pre-existing research, contribution to different fields in the literature, and an overview of the methodological approach. 6.4.4 Background and related literature The background section is divided into three parts: A quick summary on the history of generics An overview of different legal systems around generic prescription and generic substitution An institutional background, that includes: A discussion on the decision situation physicians face A list of health insurance types with their respective coverage of prescription drugs. By providing these background information Hellerstein familiarizes her readers with key aspects of her topic and creates further relevance for her research. She uses descriptive statistics on generic prescription and substitution laws for different legal systems to justify her approach to include geographical covariates in her analysis. Analogously, she argues for the importance to consider the type of insurance by discussing differences between these insurance systems, especially with regard to coverage rates. 6.4.5 Empirical motivation and summary statistics Hellerstein uses this chapter for two things: to introduce her sample and to show heterogeneities in generic prescription rates in her sample. The first part of the chapter is thereby built as follows: An introduction of the raw data An elaboration on data clearing processes A descriptive overview on the patient sample (NAMCS) and the drug-level subsample (NAMCSd). The variables used for descriptive statistics will later reappear as covariates in the analysis. The data introduction helps readers to understand the sample Hellerstein uses for her analysis; how it originates, what its properties are and how valid it might be for this particular research topic. The remaining part of the chapter emphasises on descriptive differences in generic prescription rates. The heterogeneities Hellerstein focusses on are in particular: In generic prescription rates across drug classes. How frequent physicians prescribe generics. Whether physicians prescribe the same medication sometimes as generic and sometimes as trade-name version, or whether they stick with one option all the time. These patterns lay the foundation on why her empirical analysis is important. Further, the statistics in this chapter introduce and justify the inclusion of covariates in her model. 6.4.6 The model Hellerstein clarifies from a theoretical perspective, what factors might determine a physicians decision to prescribe a generic. Beyond clarifying the decision situation Hellersteins analysis revolved around the model also serves as a hypothesis for her research question. In the formula, it is clearly defined when the insurance coverage (that closely correlates with the insurance status) nudges a physician towards prescribing a generic. Last, the model serves as an additional argument, what variables should be considered when empirically modelling the behavior of physicians. 6.4.7 Empirical implementation In the first part of this chapter, Hellerstein elaborates how her theory lays framework for empirical application. Most parameters of her theoretical model are not included in the NAMCS data. She therefore discusses how to proxy missing elements using available variables. Next, she introduces her empirical model. First, she describes all variables and variable vectors that are included. Next, she discusses her choice of using a random effects estimator. The chapter concludes with a discussion of limitations and possible extensions to her empirical approach. 6.4.8 Estimation results In the results chapter, Hellerstein again describes her regression model and presents the results. Further she discusses concerns about her estimates and uses other specifications of her model to show the robustness of her results. Some results of the robustness tests are also reported. The chapter starts with a re-specification of her empirical application. It is noteworthy, that the model described here deviates from the equation presented in the previous chapter. All estimated coefficients of this single regression are reported in 3 separate tables: Table 4: Coefficients of demographic variables and frequency of demographics per physician Table 5: Coefficients of drug class dummies Table 6 Coefficients of drug class x insurance type interactions. In the text accompanying the tables, she picks numerous coefficients, provides possible explanations, and derives implications. Hellerstein mentions multiple limitations of her model and addresses concerns by a number of robustness checks. These include: Using a subsample of patients below age 65. Using a subsample of physicians with more than 20/30 patients in the overall NAMCS (results for both of these robustness tests are not reported). Using a subsample of observations from states with a special legal system (these results are reported). Hellerstein not only uses the results section to report her findings, but also to discuss their implications and limitations. Typically, this is done in a separate discussion section 6.4.9 Conclusion Hellerstein starts her conclusion with a short summary of the research findings: What is the aim of her analysis? What are the key findings? What is the takeaway? Hellerstein then finalizes her article with a discussion on how findings are relevant for future research and policy. 6.4.10 Appendices The paper appendix consists of two parts: A methodological background on the random effects estimator Hellerstein uses Additional background information on the data and the data clearing processes 6.4.11 References Hellerstein lists all in-text references used in the paper in alphabetical order. References "],["replication-results-of-hellerstein-1998.html", "Chapter 7 Replication results of Hellerstein (1998) 7.1 Descriptive Analysis 7.2 Empirical Analysis", " Chapter 7 Replication results of Hellerstein (1998) We show the results of the replication of Tables 1-3 and Figures 1-2 of the descriptive analysis, and the replication of the empirical analysis in Tables 4-6 in this document). Our results are based on data from 1991 which is in contrast to Hellerstein (1998) that used three datasets from 1989. As access to confidential data is required to identify physicians and patients in the 1989 data, we resort to 1991 that provide state identifiers in two publicly available datasets. We present our results and discuss deviations from Hellerstein (1998). A detailed discussion and interpretation is provided in the original study. 7.1 Descriptive Analysis Summary statistics are shown in Table 1. They represent the sample of patients represented in NAMCS in the year 1991. Results are similar to the summary statistics shown by Hellerstein (1998). Some categories of the payment sources differ slightly. Four categories  Self-pay, Medicare, Madicaid and HMO/prepaid plan  match. The categories Blue Cross/ Blue Shield and Other commerical insurer seem to have merged into the category Private/Commercial. When looking at the distribution of insurance coverage, we see that the frequency of patients with status Private/Commercial adds up to the frequencies of Blue Cross and other commercial insurers. Differences in the distribution of other covariates are slim. The most noticable difference lies in the share of specialist physicians, which account for 68% of visited physicians in the 1991 sample compared to 55% in 1989. Hellerstein Table 1 - Summary Statistics for Overall NAMCS Patient Sample Variable mean sd Age 43.07 24.81 Female 0.59 0.49 Nonwhite 0.11 0.31 Hispanic 0.06 0.23 Self-pay 0.22 0.41 Medicare 0.14 0.35 Medicaid 0.10 0.30 Private/Commercial 0.37 0.48 Other government insurance 0.02 0.15 HMO/prepaid plan 0.15 0.36 Specialist 0.68 0.46 Northeast 0.23 0.42 Midwest 0.25 0.44 South 0.28 0.45 West 0.24 0.43 Notes: Data source: NAMSC91. Sample size is 29,854. Sample size differs as we use data from a different year. However, the public data from 1989 (the one Hellerstein uses) allow for reproduction of Table 1. For further information see Table 1 notes in Hellerstein (1998); With the specifications stated in the manuscript, we cannot reproduce table 1 completely. Table 2 shows very similar statistics for the patients of the NAMCSd sample compared to the 1989 data. We observe a smaller sample size in the 1991 data. Hellerstein Table 2 - Summary Statistics for Patients in NAMCS Drug Sample Mean Standard Deviation Proportion Generic Age 43.79 25.13 Female 0.59 0.49 0.27 Nonwhite 0.12 0.32 0.34 Hispanic 0.06 0.24 0.33 Self-Pay 0.27 0.44 0.29 Medicare 0.15 0.36 0.21 Medicaid 0.11 0.31 0.32 Private/Commercial 0.33 0.47 0.27 HMO/prepaid plan 0.15 0.35 0.34 Specialist 0.60 0.49 0.26 Northeast 0.21 0.41 0.28 Midwest 0.27 0.44 0.27 South 0.29 0.46 0.25 West 0.23 0.42 0.35 Full sample 0.28 Notes: The sample size is 7,715. For further notes see Hellerstein (1998), Data source: NAMSCd91. Table 3 shows the absolute number of observations and the share of generics over all drugs as well as the eight largest drug categories. We see an overall similar generic share over all drug classes compared to Hellerstein (1998). Yet in the drug class pain relief we see an increase of generic share by about \\(9\\) percentage points. Hellerstein Table 3 - Frequency of Generic Prescription by Drug Class Observations % Generics All drugs 7715 28.37 By drug class Antimicrobials 2955 40.37 Cardiovascular-renals 1344 16.15 Central Nervous System 789 25.48 Hormones/Hormonal mechanisms 917 35.66 Skin/Mucous membrane 530 9.06 Ophthalmics 295 13.90 Pain relief 634 21.29 Respiratory tract 251 10.76 The distribution of physicians over the different generic prescription rates (Figure 5.1) is similar to Hellerstein (1998). Figure 7.1: Hellerstein Figure 1 - Distribution of Physician Generic Prescription Rates (Source: NAMSCd91) Figure 2 Figure 7.2: Hellerstein Figure 2 - Physician decisions by physicians who prescribe a drug to at least six patients (Source: NAMSCd91) When looking at Figure 5.2, we see a decrease from about 90% to about 50% in only trade-name prescriptions by physicians. This seems to be driven by a rather small increase of prescription of both versions and only generics. 7.2 Empirical Analysis Keep in mind that due to data restrictions, we were not able to control for certain covariates that Hellerstein (1998) includes. An example of such are legislation laws like mandatory or permissive substitution, and one- or two-line prescription. For this reason, we cannot present all tables produced by Hellerstein (1998). Given these restrictions, we cannot run the analysis with the same underlying model specifications. This restricts us from comparing the results from 1991 and 1989. However, for the completion of this example we present the results from Tables 4, 5 and 6: Hellerstein Table 4 - Estimated Coefficients on Demographic Variables, Geographic Variables, and Average Characteristics for Full Sample, excluding regional identifiers Random-Effects Probit Coefficient Percent change in Generic Constant -0.556** (-3.17) Age -0.002* (-1.99) -0.001* (-1.99) Female -0.112** (-2.94) -0.028** (-2.94) Hispanic 0.025 (0.28) 0.006 (0.28) Nonwhite 0.044 (0.62) 0.011 (0.62) Specialist 0.019 (0.25) 0.005 (0.25) Mean age -0.003 (-1.10) -0.001 (-1.10) Percent female -0.189 (-1.16) -0.048 (-1.16) Percent black 0.075 (0.41) 0.019 (0.41) Percent Hispanic -0.112 (-0.45) -0.028 (-0.45) Percent Medicaid 0.062 (0.26) 0.016 (0.26) Percent Medicare -0.155 (-0.71) -0.039 (-0.71) Percent private insured 0.155 (1.08) 0.039 (1.08) Percent HMO/prepaid 0.054 (0.30) 0.014 (0.30) Midwest -0.141 (-1.53) -0.036 (-1.53) South -0.206* (-2.28) -0.052* (-2.29) West 0.065 (0.67) 0.016 (0.67) Notes: \\(*p &lt; 0.05, **p &lt; 0.01, ***p &lt; 0.001\\); The sample size is 7,715. For further notes see Hellerstein (1998); Data source: NAMSCd91. Table 5 shows the estimated coefficients for the eight largest drug classes as well as the % change in the generic share. The greatest change compared to Hellerstein (1998) is the change in sign from positive to negative for cardiovascular/renals, though not significant in our model. Hellerstein Table 5 - Estimated Coefficients for Drug-Class Dummy Variable for Full Sample Random-Effects Probit Coefficient % Change in Generic main Antimicrobials 0.628*** (8.47) 0.159*** (8.54) Cardiovascular-renals -0.147 (-1.71) -0.0371 (-1.71) Central Nervous System 0.0559 (0.58) 0.0141 (0.58) Hormones/Hormonal mechanisms 0.584*** (6.92) 0.148*** (6.95) Skin/Mucous membrane -0.731*** (-6.26) -0.185*** (-6.28) Ophthalmics -0.383* (-2.47) -0.0968* (-2.47) Resperatory tract -0.675*** (-4.79) -0.171*** (-4.80) Notes: \\(*p &lt; 0.05, **p &lt; 0.01, ***p &lt; 0.001\\); The sample size is 7,715. Omitted category is pain relief. For further notes see Hellerstein (1998); Data source: NAMSCd91. Hellerstein Table 6 - Tests for Moral Hazard for the Full Sample Equality of Individual Insurance Variables with Self-Payment Random-Effects Probit Results Antimicrobial % change Cardiovasculars % change Central Nervous System % change Hormones % change Skin/Muchous Membranes % change Ophthalmics % change Pain relief % change Respiratory Tract % change Medicaid Coefficient 0.14 0.04 0.00 0.00 0.02 0.00 0.38 0.08 0.03 0.00 -0.25 -0.05 0.54 0.08 -1.21 -0.09 1.12 1.13 0.02 0.02 0.08 0.08 1.23 1.23 0.04 0.04 -0.60 -0.60 1.12 1.15 -1.08 -1.11 Medicare Coefficient 0.11 0.03 0.05 0.01 -0.32 -0.06 -0.42 -0.09 0.54 0.05 -0.41 -0.08 0.08 0.01 -0.60 -0.05 0.72 0.72 0.25 0.25 -1.13 -1.12 -1.68 -1.69 0.89 0.90 -0.97 -0.97 0.18 0.18 -0.67 -0.68 Private Coefficient 0.00 0.00 -0.06 -0.01 -0.06 -0.01 -0.16 -0.03 0.03 0.00 -0.31 -0.06 0.18 0.03 -1.27 -0.10 0.01 0.01 -0.30 -0.30 -0.28 -0.28 -0.75 -0.75 0.07 0.07 -0.91 -0.91 0.51 0.51 -1.45 -1.56 HMO/Prepaid Coefficient 0.22 0.06 -0.14 -0.03 0.32 0.06 -0.33 -0.07 -0.42 -0.04 -0.57 -0.11 0.28 0.04 -0.26 -0.02 1.78 1.78 -0.56 -0.56 1.04 1.05 -1.09 -1.09 -0.73 -0.73 -0.93 -0.93 0.71 0.72 -0.26 -0.25 Notes: see Hellerstein (1998); Data source: NAMSCd91. References "],["stata-replication-code-of-hellerstein-1998.html", "Chapter 8 Stata replication code of Hellerstein (1998) 8.1 Preparing the analysis data set 8.2 Descriptive statistics 8.3 Empirical analyses and tables", " Chapter 8 Stata replication code of Hellerstein (1998) The following Stata program replicates the empirical analyses of Hellerstein, Judith K. 1998. The Importance of the Physician in the Generic versus Trade-Name Prescription Decision, The RAND Journal of Economics 29(1):10836, doi: 10.2307/2555818. This file contains program code for data preparation, replication of descriptive tables and figures, and replication of the empirical analyses and tables. The structure of this file is as follows: Preparing the NAMCS and NAMCSd data (Section 4) Descriptive statistics using NAMCS and NAMCSd data (Section 5) Empirical analyses using NAMCSd data (Section 5 and Section 6) 8.1 Preparing the analysis data set * Pathing and setup log close _all snapshot erase _all clear all *storage of NAMCsd raw data global r `&quot; &quot;C:\\Users\\my name\\my project\\raw data\\&quot; &quot;&#39; *analysis data global d `&quot; &quot;C:\\Users\\my name\\my project\\data\\&quot; &quot;&#39; *results global tab `&quot; &quot;C:\\Users\\my name\\my project\\tables\\&quot; &quot;&#39; global fig `&quot; &quot;C:\\Users\\my name\\my project\\figures\\&quot; &quot;&#39; *Data were downloaded from https://ftp.cdc.gov/pub/Health_Statistics/NCHS/namcs_public_use_files/* *1991 datasets and documentation: namcs91.exe; namcs91d.exe; namcs91doc.pdf* * Preparing NAMCS91: ****************************** use &quot;$r\\namcs91_raw.dta&quot; , clear * 1) Set up relevant dummies gen female = sex == 1 gen nonwhite = race != 1 gen hispanic = ethnicity == 1 gen northeast = geo_reg == 1 gen midwest = geo_reg == 2 gen south = geo_reg == 3 gen west = geo_reg == 4 //Hellerstein defines specialists as //physicians who are not in general practice, family practice, or basic pediatrics. gen specialist = (phys_special != &quot;GP&quot; &amp; phys_special != &quot;FP&quot; &amp; phys_special != &quot;PD&quot;) //Medicare patients gen temp = selfpay + medicaid + other_gov_ins + private_ins + hmo_pre_paid replace medicare = 0 if temp != 0 * 2) Drop observations with unknown payment gen temp1 = selfpay + medicaid + medicare + other_gov_ins + private_ins + hmo_pre_paid keep if temp1 == 1 compress save &quot;$d\\namcs91.dta&quot;, replace * Preparing NAMCS91d: **************************** use &quot;$r\\namcs91d_raw.dta&quot; , clear * 1) Removing observations with missing values: *********************************************** replace generic_id = .b if generic_id == 50000 replace generic_status = .b if generic_status == 3 replace prescription_status = .b if prescription_status == 3 replace composition_status = .b if composition_status == 3 | composition_status == 6 drop if missing(generic_id)|missing(generic_status)|/// missing(prescription_status)|missing(composition_status) * 2) Creating required variables: ********************************** ** 2.1) Define 8 largest drug classes, see Table 3 and page 79 in Hellerstein&#39;s paper gen major_drug_class = /// drug_class == 3 | drug_class == 5 | drug_class == 6 | drug_class == 10 | /// drug_class == 12 | drug_class == 15 | drug_class == 17 | drug_class == 19 ** 2.2) Creating some dummies gen female = sex == 1 gen nonwhite = race != 1 gen hispanic = ethnicity == 1 gen northeast = geo_reg == 1 gen midwest = geo_reg == 2 gen south = geo_reg == 3 gen west = geo_reg == 4 gen specialist = (phys_special != &quot;GP&quot; &amp; phys_special != &quot;FP&quot; &amp; phys_special != &quot;PD&quot;) gen temp = selfpay + medicaid + private_ins + hmo_pre_paid replace medicare = 0 if temp != 0 replace generic_status = 0 if generic_status == 2 replace prescription_status = 0 if prescription_status == 2 tab drug_class, gen (drug_class_) label variable drug_class_3 &quot;Antimicrobial&quot; label variable drug_class_5 &quot;Cardiovascular-renals&quot; label variable drug_class_6 &quot;Central Nervous System&quot; label variable drug_class_10 &quot;Hormones&quot; label variable drug_class_12 &quot;Skin/Mucous Membranes&quot; label variable drug_class_15 &quot;Ophthalmics&quot; label variable drug_class_17 &quot;Pain Relief&quot; label variable drug_class_19 &quot;Respiratory Test&quot; ** 2.3) Create multisource indicator *** 2.3.1) Create id for drugs with multiple ingrediants replace ingredients = generic_id if ingredients == . *** 2.3.2) Check whether there are both generic and trade-name drugs for each ingredients bys ingredients: egen counter = mean(generic_status) bys ingredients: gen multisource = (counter &gt;0 &amp; counter &lt;1) drop counter ** 2.4) Set up physician averages foreach x in age female nonwhite hispanic medicare medicaid hmo_pre_paid private_ins{ cap: drop mean_`x&#39; bys physician_id: egen mean_`x&#39; = mean(`x&#39;) } * 3) Keeping relevant variables **************************** //Dropping conditions keep if prescription_status == 1 &amp; multisource == 1 &amp; major_drug_class == 1 &amp; order_of_drug == 1 * (selfpay == 1 | medicaid == 1 | private_ins == 1 | hmo_pre_paid == 1 | medicare == 1) gen temp1 = selfpay + medicaid + medicare + private_ins + hmo_pre_paid keep if temp1 == 1 keep /// drug_id drug_name generic_id generic_name generic_status drug_class drug_class_* ingredients /// age female nonwhite hispanic west northeast midwest south geo_reg specialist mean_* /// selfpay medicare medicaid private_ins hmo_pre_paid /// geo_reg phys_special phys_type physician_id patient_id compress save &quot;$d\\namcs91d.dta&quot;, replace end of do-file 8.2 Descriptive statistics * Table 1: **************** /* Comment: Information was obtained from the footnote of Table 1, p. 115 of the original study. The text stats that only uses data without missing values. This applies to all tables and figures and was done in the initial data preparation. The footnote defines the dummy variable &quot;specialist&quot;: whether a physician is not a general practice, family practice, or basic pediatrics. The dummy medicare takes the value of 1 if medicare was the only source of payment. Observations of patient visits in which the patient was not charged were excluded. */ use &quot;$d\\namcs91.dta&quot;, clear qui eststo table1: estpost summarize age female nonwhite hispanic selfpay medicare medicaid /// private_ins other_gov_ins hmo_pre_paid specialist northeast midwest south west esttab table1 using &quot;$tab\\Table_1.rtf&quot;, replace /// title(&quot;Summary Statistics for Overall NAMCS Patient Sample&quot;) /// cells (&quot;mean(fmt(%12.2f)) sd(fmt(%12.2f))&quot;) nonumber noobs /// coeflabels(age &quot;Age&quot; female &quot;Female&quot; nonwhite &quot;Nonwhite&quot; hispanic &quot;Hispanic&quot; /// selfpay &quot;Self-pay&quot; medicare &quot;Medicare&quot; medicaid &quot;Medicaid&quot; private_ins &quot;Private/commercial&quot; /// other_gov_ins &quot;Other government insurance&quot; hmo_pre_paid &quot;HMO/prepaid plan&quot; /// specialist &quot;Specialist&quot; /// northeast &quot;Northeast&quot; midwest &quot;Midwest&quot; south &quot;South&quot; west &quot;West&quot; ) /// addnote(&quot;Notes: Sample size is 32,407. For further notes see table 1 notes in Hellerstein (1998); /// Sample size differs as data from different years is used&quot; &quot;Data source: NAMSC91&quot;) * Table 2: **************** /* Comment: Variables and data preparing processes equivalent to Table 1. Create a matrix resembling the table and store each value in its respective cell. */ use &quot;$d\\namcs91d.dta&quot;, clear * 1) Saving all values in locals: ********************************** * For all rows besides the last one foreach x in age female nonwhite hispanic selfpay medicare medicaid private_ins hmo_pre_paid specialist northeast midwest south west { * Computing mean and sd qui: sum `x&#39; local `x&#39;_m = r(mean) local `x&#39;_std = r(sd) * Computing the share of generics for all rows exept for the row age if &quot;`x&#39;&quot; != &quot;age&quot;{ qui: sum generic_status if `x&#39; == 1 local `x&#39;_share = r(mean) } } * Last row (Share of generics in the full sample) qui: sum generic_status local fullsample_share = r(mean) * 2) Filling a matrix with the locals: *************************************** mat input table2 = ( /// `age_m&#39;, `age_std&#39; , . \\ /// `female_m&#39;, `female_std&#39;, `female_share&#39; \\ /// `nonwhite_m&#39;, `nonwhite_std&#39;, `nonwhite_share&#39; \\ /// `hispanic_m&#39;, `hispanic_std&#39;, `hispanic_share&#39; \\ /// `selfpay_m&#39;, `selfpay_std&#39;, `selfpay_share&#39; \\ /// `medicare_m&#39;, `medicare_std&#39;, `medicare_share&#39; \\ /// `medicaid_m&#39;, `medicaid_std&#39;, `medicaid_share&#39; \\ /// `private_ins_m&#39;, `private_ins_std&#39;, `private_ins_share&#39; \\ /// `hmo_pre_paid_m&#39;, `hmo_pre_paid_std&#39;, `hmo_pre_paid_share&#39; \\ /// `specialist_m&#39;, `specialist_std&#39;, `specialist_share&#39; \\ /// `northeast_m&#39;, `northeast_std&#39;, `northeast_share&#39; \\ /// `midwest_m&#39;, `midwest_std&#39;, `midwest_share&#39; \\ /// `south_m&#39;, `south_std&#39;, `south_share&#39; \\ /// `west_m&#39;, `west_std&#39;, `west_share&#39; \\ /// . , . , `fullsample_share&#39; /// ) * 3) Labelling and exporting: ************************************** matrix rownames table2 = /// &quot;Age&quot; &quot;Female&quot; &quot;Nonwhite&quot; &quot;Hispanic&quot; &quot;Self-Pay&quot; &quot;Medicare&quot; &quot;Medicaid&quot; &quot;Private/Commercial&quot; /// &quot;HMO/prepaid plan&quot; /// &quot;Specialist&quot; &quot;Northeast&quot; &quot;Midwest&quot; &quot;South&quot; &quot;West&quot; &quot;Full sample&quot; matrix colnames table2 = &quot;Mean&quot; &quot;Standard Deviation&quot; &quot;Proportion Generic&quot; putexcel set &quot;$tab\\Table_2.xlsx&quot;, replace putexcel A1 = matrix(table2 ), names * Table 3: **************** use &quot;$d\\namcs91d.dta&quot;, clear return clear ereturn clear * Turn decimal numbers into percentage gen All_drugs = generic_status * 100 * generic share for all drugs eststo t10: quietly estpost tabstat All_drugs, /// statistics(mean N) columns(statistics) listwise * generic share by drug class eststo t20: quietly estpost tabstat All_drugs, by(drug_class) /// statistics(mean N) columns(statistics) listwise notot esttab t10 t20 using &quot;$tab\\Table_3.rtf&quot;, replace /// cells(&quot;count(fmt(%12.0f)) mean(fmt(%12.2f))&quot;) /// title(&quot;Table 3 - Frequency of Generic Prescription by Drug Class&quot;) /// collabels(&quot;Observations&quot; &quot;% Generics&quot;) noobs nonumber gaps refcat(3 &quot;By drug class&quot; , nolabel) /// coeflabels(generic &quot;All drugs&quot; 3 &quot;Antimicrobials&quot; 5 &quot;Cardiovascular-renals&quot; /// 6 &quot;Central Nervous System&quot; 10 &quot;Hormones/Hormonal mechanisms&quot; /// 12 &quot;Skin/Mucous membrane&quot; 15 &quot;Ophthalmics&quot; 17 &quot;Pain relief&quot; 19 &quot;Resperatory tract&quot;) * Figure 1: ***************** /* Comment: Visualizations of kernel densities vary depending on the specifications. Two parameters define the density function: Bandwidth and whether/how to round the data. Hellerstein does not provide information about the density function. We visually rule out that she rounds to the first decimal place. There are fluctuations between the first decimal places. Rounding to the third or higher decimal places produces much noisier curves. Hence, we assume that she rounded to the second decimal place. For the bandwidth and density function, there are no obvious clues. That the reproduction relies on visual approximation of the original figure. */ * 1) Estimate the mean share of prescribed generics per physician bys physician_id: egen temp2 = mean(generic_status) gen generic_share = round(temp2, .01) * 2) Collapse the data so that one observation per physician remains duplicates drop physician_id, force * 3) Recreate Figure 1 set scheme s1manual twoway kdensity generic_share, range(0 1) bw( 0.02) kernel(bi) /// xtitle(&quot;Physician generic prescription rates&quot;) ytitle(&quot;Percent of physicians&quot;) /// xlabel(0 (0.1) 1) lc(black) graph export &quot;$fig\\Figure_1.png&quot;, replace * Figure 2: ***************** use &quot;$d\\namcs91d.dta&quot;, clear /* Comment: The figure uses observations of drugs with &gt; 6 prescriptions by a single physician. 158 unique physicians remain in Hellerstein (1998). We are left with 122 physicians (see p. 116 first paragraph). */ * 1) Apply dropping conditions bys physician_id ingredient : gen temp = _N drop if temp &lt; 6 * Check the number of remaining physicians. 158 in Hellerstein qui: tab physician_id di r(r) * 2) Create the categories for the bars ** 2.1) Avg generic prescription rate for each per physician bys physician_id ingredients: egen temp2 = mean(generic_status) * 2.2) Use the share to set up the groups gen counter = . replace counter = 0 if temp2 == 0 /* Never generic */ replace counter = 1 if temp2 &gt; 0 &amp; temp2 &lt; 1 /* Sometimes */ replace counter = 2 if temp2 == 1 /* Always */ label define l 0 &quot;Only trade names&quot; 1 &quot;Both versions&quot; 2 &quot;Only generics&quot; label values counter l * 3) Collapse the data so that one observation per physician remains duplicates drop physician_id, force * 4) Recreating the figure set scheme s1manual graph bar (count), over(counter) /// ytitle(&quot;&quot;) intensity(60) lintensity(100) bar(1, color(&quot;black&quot;) fcolor(&quot;gs5&quot;)) ylab(, nogrid) graph export &quot;$fig\\Figure_2.png&quot;, replace invalid &#39;C&#39; r(198); end of do-file r(198); 8.3 Empirical analyses and tables * Setting up the regression input, equivalent to the notation in the paper use &quot;$d\\namcs91d.dta&quot;, clear xtset physician_id gen G = generic_status /* drug classes */ global C drug_class_3 drug_class_5 drug_class_6 drug_class_10 drug_class_12 drug_class_15 drug_class_19 /* patient demographics */ global X age female hispanic nonwhite /* insurance */ global P medicare medicaid hmo_pre_paid private_ins /* is phys a specialist */ global S specialist /* region */ global R midwest south west /* patient avgs */ global X_dash mean_age mean_female mean_nonwhite mean_hispanic /* insurance avgs */ global P_dash mean_medicaid mean_medicare mean_private_ins mean_hmo_pre_paid * M and T are not included due to missing state id * Table 4: **************** return clear ereturn clear eststo clear eststo table4: xtprobit G $C $X $P $S $X_dash $P_dash $R, re eststo marginstable4: margins, dydx( $X $S $X_dash $P_dash $R) post *Reporting Table 4 esttab table4 marginstable4 using &quot;$tab\\Table-4.rtf&quot;, replace wide /// keep(_cons $X $S $X_dash $P_dash $R) stats(rho) noobs b(3) order(_cons) /// coeflabels(_cons &quot;Constant&quot; age &quot;Age&quot; female &quot;Female&quot; nonwhite &quot;Nonwhite&quot; hispanic &quot;Hispanic&quot; /// specialist &quot;Specialist&quot; mean_age &quot;Mean age&quot; /// mean_female &quot;Percent female&quot; mean_nonwhite &quot;Percent black&quot; /// mean_hispanic &quot;Percent Hispanic&quot; mean_medicaid &quot;Percent Medicaid&quot; mean_medicare /// &quot;Percent Medicare&quot; mean_private_ins &quot;Percent private insured&quot; /// mean_hmo_pre_paid &quot;Percent HMO/prepaid&quot; northeast &quot;Northeast&quot; midwest /// &quot;Midwest&quot; south &quot;South&quot; west &quot;West&quot;) /// mtitles(&quot;Random-Effects Probit Coefficient&quot; &quot;% Change in Generic&quot;) nonumber addnote(&quot;Datasource: NAMSC91&quot;) * Table 5: **************** return clear ereturn clear eststo clear eststo table5: xtprobit G $C $X $P $S $R $X_dash $P_dash, re eststo marginstable5: margins, dydx($C) post *Reporting Table 5 esttab table5 marginstable5 using &quot;$tab\\Table-5.rtf&quot;, replace noobs wide keep($C) nonumber /// coeflabels(drug_class_1 &quot;Antimicrobials&quot; drug_class_2 &quot;Cardiovascular-renals&quot;/// drug_class_3 &quot;Central Nervous System&quot; drug_class_4 /// &quot;Hormones/Hormonal mechanisms&quot; drug_class_5 &quot;Skin/Mucous membrane&quot; drug_class_6 &quot;Ophthalmics&quot; drug_class_8 &quot;Resperatory tract&quot;) /// addnote(&quot;Notes: The ommitted drug category is pain relief&quot; &quot;Datasource: NAMSC91&quot;) mtitles(&quot;Random-Effects Probit Coefficient&quot; &quot;% Change in Generic&quot;) * Table 6: **************** return clear ereturn clear eststo clear foreach x in 3 5 6 10 12 15 17 19 { * Estimating and saving the coefs eststo c_`x&#39;: quietly xtprobit G $X $P $S $R $X_dash $P_dash if drug_class_`x&#39; == 1, re * Estimating and saving the marginal effects (AME, see table footer) eststo m_`x&#39;: qui margins, dydx($P) post } esttab c_3 m_3 c_5 m_5 c_6 m_6 c_10 m_10 c_12 m_12 c_15 m_15 c_17 m_17 c_19 m_19 using &quot;$tab\\Table6.rtf&quot;, replace /// nostar nopar keep($P) noobs b(2) order(medicaid medicare private_ins hmo_pre_paid) /// refcat(medicare &quot;Medicare&quot; medicaid &quot;Medicaid&quot; hmo_pre_paid &quot;HMO/Prepaid&quot; private_ins &quot;Private&quot;, nolabel) /// coeflabels(medicare &quot;Coefficient&quot; medicaid &quot;Coefficient&quot; hmo_pre_paid &quot;Coefficient&quot; private_ins &quot;Coefficient&quot;) /// mtitle(&quot;Antimicrobial&quot; &quot;% change&quot; &quot;Cardiovasculars&quot; &quot;% change&quot; &quot;Central Nervous System&quot; &quot;% change&quot; &quot;Hormones&quot; &quot;% change&quot; &quot;Skin/Muchous Membranes&quot; &quot;% change&quot; &quot;Ophthalmics&quot; /// &quot;% change&quot; &quot;Pain relief&quot; &quot;% change&quot; &quot;Respiratory Tract&quot; &quot;% change&quot; ) nonumbers eqlabels(none) collabels(none) nonumbers compress * Tables 7, 8 and 9 are not reproducible due to missing state id in the publicly available datasets invalid &#39;C&#39; r(198); end of do-file r(198); "],["references-1.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
